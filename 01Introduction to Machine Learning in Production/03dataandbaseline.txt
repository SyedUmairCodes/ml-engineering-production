# Why is data definition hard?
Welcome back. You're now in the 3rd and final week of this course, just one more week, and then you'll be done with this 1st course of the specialization. In this week we dive into data. How do you get data that sets up your training, your modeling for success? But first, why is defining what data to use even hard? Let's look at an example. I'm going to use the example of detecting Iguanas. One of my friends, [inaudible] really likes Iguanas, so I have a bunch of iguana pictures floating around. Let's say that you've gone into the forest and collected hundreds of pictures like these and you send these pictures to labelers with the instructions, "Please use bounding boxes to indicate the position of Iguanas.'' One labeler, may label it like this and say, one iguana, two Iguanas. This labeler did a good job. A 2nd labeler that is equally hard working, equally diligent may say, look, the iguana on the left has a tail that goes all the way to the right to this image. The 2nd labeler may say one iguana, two iguanas. Good job, labeler. Hard to follow this labor either. A 3rd labeler may say, well, I'm going to look through all hundreds of images and label them all, and I'm going to use bounding boxes, and so let me indicate the position iguanas and draw a bounding box like that. Three diligent, hard working labelers can come up with these three very different ways of labeling iguanas, and maybe any of these is actually fine. I would prefer the top two rather than the 3rd one. But any of these labeling conventions could result in your learning algorithm learning a pretty good iguana detector. But what is not fine is if 1/3 of your labelers use the 1st and 1/3 the 2nd, and 1/3, the 3rd labeling convention, because then your labels are inconsistent, and this is confusing to the learning algorithm. While, the iguana example was a fun one. You see this type of effect in many practical computer vision problems as well. Let's use the phone defect detection example. If you ask a labeler to use bounding boxes to indicate significant defects, maybe one labeler will look and then go, ''Well, clearly the scratch is the most significant defect. Let me draw a bounding box on that.'' A 2nd labeler may look at his phone and say, "There are actually two significant defects. There's a big scratch, and then there's that small mark there,'' it's called a pit mark, like if someone poked a phone with a sharp screwdriver. I think the 2nd labeler probably did a better job. But then a 3rd labeler may look at this and say, well, here's a bounding box that shows you where the defects are. Between these three labels, probably the one in the middle would work the best. But this is a very typical example of inconsistence labeling that you will get back from a labeling process with even slightly ambiguous labeling instructions, and if you can consistently label the data with one convention, maybe the one in middle, you're learning algorithm will do better. What we would do in this week is dive into best practices for the data stage of the full cycle of a machine learning project. Specifically, we'll talk about how to define what is the data, what should be x and what should be y and establish a baseline and doing that well will set you up to label and organize the data well, which would give you a good data set for when you move into the modeling phase, which you already saw last week. Many machine learning researchers and many machine learning engineers had started off downloading data off the Internet to experiment with models, so using data prepared by someone else. Nothing at all wrong with that, and for many practical applications, the way you prepare your data sets will have a huge impact on the success of your machine learning projects. In the next video, we'll take a look at some more examples of how data can be ambiguous, so that this will set us up later this week for some techniques for improving the quality of your data. Let's go on to the next video.

# More label ambiguity examples
In the last video, you saw how the right bounding boxes for an image can be ambiguous. Let's take a look at some more label ambiguity examples. We briefly touched on speech recognition in the first week of this course. Here's another example. Given this audio clip,
Play video starting at ::24 and follow transcript0:24
sounds like someone was standing on a busy road side asking for the nearest gas station and then a car drove past. Did they say something right after that? I don't know. One way to transcribe this would be "Um, nearest gas station." In some places, people spell "um" with two m's. That would be a different way to spell it. We could have used dot-dot-dot or ellipses instead of the comma as well, which would be another ambiguity. Or given the audio had noise after the last words. Nearest gas station. Did they say something after nearest gas station? I'm not sure actually. Would you transcribe it like this instead? There are combinatorially many ways to transcribe this. With one M or two M's, comma or ellipses, whether to write unintelligible at the end of this. Being able to standardize on one convention will help your speech recognition algorithm. Let's also look an example of structured data. A common application in many large companies is user ID merge. That's when you have multiple data records that you think correspond to the same person and you want to merge these user data records together. For example, say you run a website that offers online listings of jobs. This may be one data record that you have from one of your registered users with the email, first name, last name and address. Now, say your company acquires a second company that runs a mobile app that allows people to login, to chat and get advice from each other about their resumes. It seems synergistic for your business. If you run a listing of online jobs, maybe you merge or acquire a second company that runs a mobile app that lets people chat about their resumes and from this mobile app, you have a different database of users. Given this data record and this one, do you think these two are the same person? One approach to the User ID merge problem, is the use of supervised learning algorithm that takes as inputs to user data records and tries to outputs either one or zero based on whether it thinks these two are actually the same physical human being. If you have a way to get ground true data records, such as if a handful of users are willing to explicitly link the two accounts, then that could be a good set of labeled examples to train an algorithm. But if you don't have such a ground true set of data, what many companies have done is ask human labors, sometimes a product management team to just manually look at some pairs of records that have been filtered to have maybe similar names or similar ZIP codes, and then to just use human judgment to determine if these two records appear to be the same person. Because whether these two records really is the same person, is genuinely ambiguous. They may and they may not be different people will label these records inconsistently. If there's a way to just get them to label the data a little more consistently, you see some examples of how to do this later even when the ground truth is ambiguous, then that can help the performance of your learning algorithm. User ID merging is a very common function in many companies. Let me just ask you to please do this only in ways that are respectful of the users data and their privacy and only if you're using the data in a way, consistence with what they have given you permission for. User privacy is really important. A few other examples from structured data. If you are trying to use the learning algorithm to look at the user account like this and predict is it a bot or a spam account?
Play video starting at :5:4 and follow transcript5:04
Sometimes that can be ambiguous. Or if you look at a online purchase, is this a 40-length transaction? Has someone stolen accounts and is using stolen accounts to interact with your websites or to make purchases? Sometimes that too is ambiguous. Or if you look at someone's interactions with your website and you want to know, are they looking for a new job at this moment in time based on how someone behaves on a job board website or a resume chat app, you can sometimes guess if they're looking for a job, but it's hard to be sure. That's also a little bit ambiguous. In the face of potentially very important and valuable prediction tasks like these, the ground truth can be ambiguous. If you ask people to take their best guess at the ground truth label for tasks like these, giving labeling instructions that results in more consistent and less noisy and less random labels will improve the performance of your learning algorithm. When defining the data for your learning algorithm, here are some important questions. First, what is the input x? For example, if you are trying to detect defects on smart phones, for the pictures you're taking, is the lighting good enough? Is the camera contrast good enough? Is the camera resolution good enough? If you find that you have a bunch of pictures like these, which are so dark, it's hard even for a person to see what's going on. The right thing to do may not be to take this input x and just label it. It may be to go to the factory and politely request improving the lighting because it is only with this better image quality that the labor can then more easily see scratches like this and label them. Sometimes if your sensor or your imaging solution or your audio recording solution is not good enough, the best thing you could do is recognize that if even a person can't look at the input and tell us what's going on, then improving the quality of your sensor or improving the quality of the input x, that can be an important first step to ensuring your learning algorithm can have reasonable performance. For structured data problems, defining whether the features to include can have a huge impact on your learning algorithm's performance. For example, for user ID merge, if you have a way of getting the user's location, even a rough GPS location. If you have permission from the user to use that, can be a very useful tool for deciding whether two user accounts actually belong to the same person. Of course, please do this type of thing only if you have permission from the user to use their data this way. In addition to defining the input x, you also have to figure out what should be the target label y. As you've seen from the preceding examples, one key question is, how can we ensure labels give consistent labels? In the last video and this video, you saw a variety of problems with the labels being ambiguous or in some cases, the input x not being sufficiently informative, such as an image is too dark. Let's take these data issues and put them into more systematic framework. That will allow us to devise solutions in a more systematic way. Let's go on to the next video to take a look.

# Major types of data problems
I'd like to share with you a useful framework for thinking about different major types of machine learning projects. It turns out that the best practices for organizing data for one type can be quite different than the best practices for totally different types. Let's take a look at whether these major types of machine learning projects. Let's fall in this two by two grid. One axis will be whether your machine learning problem uses unstructured data or structured data. I found that the best practices for these are very different, mainly because humans are great at processing unstructured data, the images and audio and text, and not as good at processing structured data like database records. The second axis is the size of your data set. Do you have a relatively small data set? or do you have a very large data set? There is no precise definition of what exactly is small and what is large? But I'm going to use as a slightly arbitrary threshold, whether you have over 10,000 examples or not. And clearly this boundary is a little bit fuzzy and the transitions from small to big data sets is a gradual one. But I found that best practices if you have, say 100 or 1000 examples, smaller data sets is pretty different than we have a very large data set. And the reason I chose the number 10,000 is that's roughly the size beyond which it becomes quite painful to examine every single example yourself. If you have 1000 examples, you could probably examine every example yourself. But when you have, 10,000, 100,000, million examples, it becomes very time consuming for you as an individual or maybe a couple of machinery and engineers to manually look at every example. So that affects the best practices as well. Let's look at some examples. If you are training a manufacturing visual inspection from just 100 examples of stretch phones, that's unstructured data because this is image data and it's pretty small data set. If you are trying to predict housing prices based on the size of the halls and other features of the house, from just 52 examples, then there's a structured data set. We've just real value features and a relatively small data sets. If you are carrying out speech recognition from 50 million train examples, that's unstructured data. But you have a lot of data or if you are trying to recommend products. So online shopping recommendations and you have a million users in your database, then that's a structured problem with relatively large amount of data. For a lot of unstructured data problems, people, Can help you to label data and data augmentation such as synthesizing new images or synthesizing new audio. And there's some emerging techniques for synthesizing new text as well, but data augmentation can help. So for manufacturing vision inspection, you can use data augmentation to maybe generate more pictures of smart films or for speech recognition. Data augmentation can help you synthesize audio clips with different background noise. In contrast for structured data problems, it can be harder to obtain more data and also harder to use data augmentation, if only 50 houses have been so recently in that geography. Well, it's hard to synthesize new houses that don't exist or if you have a million users in your database, again, it's hard to synthesize new users that don't really exist. And it's also harder not impossible, still worth trying, but it may or may not be possible to get humans to label the data. So I find that the best practices for unstructured versus structured data are quite different. The second axis is the size of your data set. When you have a relatively small data set, having clean labels is critical. If you have 100 training examples, then if just one of the examples is mislabeled, that's 1% of your data set. And because the data set is small enough for you or a small team to go through it efficiently, it may well be aware of your while to go through that 100 examples. And make sure that every one of those examples is labelled in a clean and consistent way, meaning according to a consistent labeling standard. In contrast, if you have a million data points, it can be harder. Maybe impossible for a small machine learning team to manually go through every example. Having clean labels is still very helpful, don't get me wrong. Even when you have a lot of data, clean labels is better than non clean ones. But because of the difficulty of having the machine learning and jointly go through every example, the emphasis is on data processes. In terms of how you collect, install the data, the labeling instructions you may write for a large team of crowdsource labelers. And once you have executed some data process, such as asked a large team of laborers to label a large set of audio clips, it can also be much harder to go back and change your mind and get everything relabeled. So let's summarize or unstructured data problems. You may or may not have a huge collection of unlabeled examples x. Maybe in your factory, you actually took many thousands of images of smartphones, but you just haven't bothered to label all of them yet. This is also common in the self driving car industry, where many self driving car companies have collected tons of images of cars driving around, but just have not yet caught in that data labeled. For these structured data problems, you can sometimes get more data by taking your unlabeled data x, and asking humans to just label more of it. This doesn't apply to every problem, but for the problems where you do have tons of unlabeled data, this can be very helpful. And as we have already mentioned, data augmentation can also be helpful. For structured data problems, is usually harder to obtain more data because you only have so many users or only so many houses were so that you can collect data from. And human labeling on average is also harder, although there are some exceptions, such as in the Lost Video where you saw that we could try to ask people to label examples for the user ID merge problem. But in many cases where we ask humans to label structure data, even when there's a completely worthwhile to ask people to try to label if two records are the same person, there's more likely to be a little bit more ambiguity. But even the human labor sometimes finds it hard to be sure what is the correct label. Lastly, let's look at small versus big data where I used to slightly arbitrary threshold of whether you have more or less than say 10,000, they put training examples. For small data sets, clean labels are critical and the data set may be small enough for you to manually look through the entire data set and fix any inconsistent labels. Further, the labeling team is probably not that large, it maybe one or two or just a handful of people that created all the labels. So if you discover an inconsistency in the labels, say one person label Iguanas one way and the different person labeled Iguanas a different way. You can just get the two or three labels together and have them talk to each other and hash out and agree on one labeling convention. For the very large data sets, the emphasis has to be on data process. And if you have a 100 labelers or even more, it's just harder to get 100 people into a room to all talk to each other and hash out the process. And so you might have to rely on a smaller team to establish a consistent label definition and then share that definition with all, say 100 or more labelers and ask them to all implement the same process. I want to leave you with one last thought, which is that I found this categorization of problems into unstructured versus structured, small versus big data. I found this to be helpful for predicting not just whether data processes generalize from one to another problem, but also whether other machine learning idea is generalized from one to another. So one tip, if you are working on a problem from one of these four quadrants, then on average advice from someone that has worked on problems in the same quadrants will probably be more useful than advice from someone that's worked in a different quadrant. I found also in hiring machine learning engineers, someone that's worked in the same quadrant as the problem I'm trying to solve will usually be able to adapt more quickly to working on other problems in that quadrant. Because the instincts and decisions are more similar within one quadrant than if you shift to a totally different quadrants in discharge. I've sometimes heard people give advice like if you are building a computer vision system always get at least 1000 labor examples. And I think people that give advice like that are well meaning and I appreciate that they're trying to give good advice, but I found that advice to not really be useful for all problems. Machine learning is very diverse and it's hard to find one size fits all advice like that. I've seen computer vision problems built with 100 examples or 100 examples for a class, screen systems built with 100 million examples. And so if you are looking for advice on a machine learning project, try to find someone that's worked in the same quadrant as the problem you are trying to solve. Now we talked about one formulation of different types of machine learning problems. There's one aspect I would like to dive into with you in the next video, which is how for small data problems, having clean data is especially important. Let's take a look at the next video of why it is true.

# Small data and label consistency
In problems of a small dataset. Having clean and consistent labels is especially important. Let's start with an example. One of the things I used to do is use machine learning to fly helicopters. One things you might want to do is take us input the voltage apply to the motor or to the helicopter rotor and predict what's the speed of the rotor. You can have this type of problem, not just to find helicopters before other control problems with controlling the speed of the motor. So let's say you have a data set that looks like this where you have five examples. So a pretty small data set because this data set that is the output Y is pretty noisy, It is difficult to know what is the the function you should use to map voltage to the rotor speed in rpm.
Play video starting at ::56 and follow transcript0:56
Maybe it should be a straight line, something like that. Or maybe something like that. Or maybe it should go up and then be flat like that. Or maybe it should be a curve like that. Really hard to tell when you have a small data set. five examples in noisy labels. It's difficult to fit a function confidently. Now, if you had a ton of data, this data set is equally noisy as the one on the left, but you just have a lot more data. Then the learning algorithm can average over the noisy data sets and you can now fill a function. You're pretty confidently looks like curve should be something like that. A lot of AI had recently grown up in large consumer Internet companies which may have 100 million users or billion users and does very large data sets. And so, I think some of the practices for how to deal with small data sets have not been emphasized as much as would be needed to tackle problems where you don't have 100 million examples, but only 1000 or even fewer. So to me, the interesting case is what if you still have a small data set? Five examples same as the example on the left. But you now have clean and consistent labels. In this case you can pretty confidently fit a function through your data and with only five examples. You can build a pretty good model for predicting speed as a function of the input voltage of trained computer vision systems with just 30 images and had to work just fine. And the key is usually to make sure that the labels are clean and consistent. Let's take a look at another example of phone defect inspection, the tosses, the tickets, input pictures like these and to decide whether there is a defect or not on the phone. Now, if labeling instructions are initially unclear, then labors will label images inconsistently. It may be that when there's a giant scratch, sufficiently large one that everyone will agree as a defect, and if there's a tiny little thing that inspectors will ignore it. But there's this region of ambiguity where different inspectors will label different scratches with a length between 0.2 and 0.4 in slightly inconsistent ways. So one solution to this would be to say, why don't we try to get a lot more pictures of phones and scratches. And then see what the inspectors do and then maybe eventually we can train a neural network. They can figure out from the image what is and what isn't a scratch on average. Maybe that approach could work, but it'd be a lot of work and require collecting a lot of images. I found that it can be more fruitful to ask the inspectors to sit down and just try to reach agreement on what is the size of scratch. That would cause them to label a scratcher of a bounding box versus decide is too small and not worth bothering labeling. So in this example, if the labelers can agree that the point of transition from where little ding becomes a defect. Is a length of 0.3, then the way they label the images becomes much more consistent. And it becomes much easier for learning algorithm to take as input images like this and consistently decide whether something is a scratch of the effect.. Just to be clear. In this example, the input to the learning algorithm is images like that on the left, not the stretched length like that on the right. But the point is, if you can get inspectors to agree what is a scratch and what is in the scratch. And to define The task as putting bounding boxes around defects are over 0.3 mm in length.
Play video starting at :5:10 and follow transcript5:10
Then that will cause your images to be labeled more consistently and allow your learning album to achieve higher accuracy. Even when your data set isn't that big. So you see the couple examples now of how label consistency helps a learning algorithm. I want to wrap up this video with one more thought, which is that big data problems can have small data challenges too. Specifically problems of the large data set, but where there's a long tail of rare events in the input will have small data challenges too. For example, the large web search engine companies all have very large data sets of web search queries, but many web queries actually very rare. And so the amount of click stream data for the rare queries is actually small or take self-driving cars. Self-driving car companies tend to have very large data sets, collected from driving hundreds of thousands or millions of hours or more. But there are rare occurrences that are critical to get right to make sure a self-driving car is safe. Such as that very rare occurrence of a young child running across the highway, or that very rare occurrence of a truck parked across the highway. So even if a self driving car has a very large data set, the number of examples that may have of these rare events is actually very small. And so ensuring label consistency in terms of how these rare events are detective and labels is still very helpful for improving self-driving cars or product recommended systems. If you have a catalog of hundreds of thousands, or millions or more items or product recommendation systems. If you have an online catalog of anywhere from thousands to hundreds of thousands to sometimes even millions of catalogs to sometimes even millions of items. Then you will have a lot of products where the number sold of that item is quite small. And so the amount of data you have of users interacting with the items in the long tail is actually small. And if there's a way which is not easy, but there's a way to make sure that data is clean and consistent, then that too will help you learning algorithm. In terms of how it recommends or doesn't recommend items in the long tail where the amount of data per item will tend to be low. So when you have a small dataset label consistency is critical. Even when you have a big data set, label consistency can be very important. It's just that found it easier, on average to get to label consistency on smaller data sets than on very large ones. In the next video, we'll look at some concrete ideas and best practices for improving your data, says label consistency. Let's go on to the next video.

# Improvin label consistency
Let's take a look at some ways to improve the consistency of your labels. Here's a general process you can use. If you are worried about labels being inconsistent, find a few examples and have multiple labelers label the same example. In some cases you can also have the same labeler label an example, wait a while until they have hopefully forgotten or technical term is wash out, but have them take a break and then come back and re-label it and see if they're even consistent with themselves. When you find that there's disagreements, have the people responsible for labeling, this could be the machine label engineer, it could be the subject matter expert, such as the manufacturing expert that is responsible for labeling what is a stretch and what isn't a stretch, and/or the dedicated labelers, discuss together what they think should be a more consistent definition of a label y, and try to have them reach an agreement. Ideally, also document and write down that agreement, and this definition of y can then become an updated set of labeling instructions that they can go back to label new data or to relabel old data. During this discussion, in some cases the labelers will come back and say they don't think the input x has enough information. If that's the case, consider changing the input x. For example, when we saw the pictures of phones, they were was so dark that we couldn't even tell what was going on, that was a sign that we should consider increasing the illumination, the lighting with which the pictures were taken. But of course, I know this isn't always possible, but sometimes this can be a big help. Then all this is an iterative process. So after improve x or after improving the label instructions, you will ask the team to label more data. If you think there are still disagreements, then repeat the whole process of having multiple labelers label the same example, major disagreement and so on. Let's look at some examples. One common outcome of this type of exercise is to standardize the definition of labels. Between these ways of labeling the audio clip you heard on the earlier video, perhaps the labelers will standardize on this as the convention, or maybe they'll pick a different one and that could be okay too. But at least this makes the data more consistent. Another common decision that I've seen come out of a process like this is merging classes. If in your labeling guidelines you asked labelers to label deep scratches on the surface of the phone, as well as shallow scratches on the surface of the phone, but if the definition between what constitutes a deep scratch versus a shallow scratch, barely visible here I know, is unclear, then you end up with labelers very inconsistently labeling things as deep versus shallow scratches. Sometimes the factory does really need to distinguish between deep versus shallow scratches. Sometimes factories need to do this to figure out what was the cause of the defect. But sometimes I found that you don't really need to distinguish between these two classes, and you can instead merge the two classes into a single class, say, the scratch class, and this gets rid of all of the inconsistencies with different labelers labeling the same thing deep versus shallow. Merging classes isn't always applicable, but when it is, it simplifies the task for the learning algorithm. One of the technique I've used is to create a new class, or create a new label to capture uncertainty. For example, let's say you asked labelers to label phones as defective or not based on the length of the scratch. Here's a sequence of smartphones with larger and larger scratches. Not sure if you can see these on your display, but let me just make them a little bit more visible here. I know that all of these are really large scratches if this is a real phone you're buying. This is just for illustrative purposes. Maybe everyone agrees that the giant scratch is a defect, a tiny scratch is not a defect, but they don't agree on what's in between. If it was possible to get them to agree, then that would be one way to reduce label ambiguity. But if that turns out to be difficult, then here's another option; which is to create a new class where you now have three labels. You can say, it's clearly not a defect, or clearly a defect, or just acknowledge there's some examples are ambiguous and put them in a new borderline class. If it becomes easier to come up with consistent instructions for this three class problem, because maybe some examples are genuinely borderline, then that could potentially improve labeling consistency. Let me use speech illustration to illustrate this further. Given this audio clip, [inaudible] I really can't tell what they said. [inaudible] If you were to force everyone to transcribe it, some labelers would transcribe, "Nearly go." Some maybe they'll say, "Nearest grocery," and it's very difficult to get to consistency because the audio clip is genuinely ambiguous. To improve labeling consistency, it may be better to create a new tag, the unintelligible tag, and just ask everyone to label this as nearest [inaudible] unintelligible. This can result in more consistent labels than if we were to ask everyone to guess what they heard when it really is unintelligible. Let me wrap up with some suggestions for working with small versus big datasets to improve label consistency. We've just been talking about unstructured data or problems where we can count on people to label the data. For small datasets there's usually a small number of labelers. So when you find an inconsistency, you can ask the labelers to sit down and discuss a specific image or a specific audio clip, and try to drive to an agreement. For big datasets, it would be more common to try to get to consistent definition with a small group, and then send the labeling instructions to a larger group of labelers. One other technique that is commonly used, but I think overused in my opinion, is that you can have multiple labelers label every example and then let them vote. Voting is sometimes called consensus labeling, in order to increase accuracy. I find that this type of voting mechanism technique, it can work, but it's probably over used in machine learning today. Where what I've seen a lot of teams do is have inconsistent labeling instructions, and then try to have a lot of labelers and then voting, to try to make it more consistent. But before resorting to this, which I do use, but more of a last resort, I would use the first, try to get to more consistent label definitions, to try to make the individual labelers choices less noisy in the first place, rather than take a lot of noisy data and then try to use voting to reduce the noise. I hope that the tools you just learnt for improving label consistency will help you to get better data for your machine learning task. One of the gaps I see in the machine learning world today is that there's still a lack of tools, and there are also machine learning ops tools for helping teams to carry out this type of process more consistently and repeatedly. It's not us trying to figure this out in the Jupyter Notebook, but instead to have tools help us to detect when labels are inconsistent and to help facilitate the process in improving the quality of the data. This is something I look forward to hopefully our community working on and developing. In terms of improving label quality, one of the questions that often comes up is: What is human level performance on the task? I find human level performance to be important and sometimes misused concept. Let's take a deeper look at this in the next video.

# uman label performance
Some machine learning tasks are trying to predict an inherently ambiguous output and Human Level Performance can establish a useful baseline of performance as a reference. But Human Level Performance is also sometimes misuse. Let's take a look. One of the most important users of measuring Human Level Performance or HLP is to estimate based error or irreducible error. Especially on unstructured data tasks in order to help with their analysis and prioritization and just establish what might be possible. Take a visual inspection tasks. This may have happened to you before, but I have gotten requests from business owners saying, hey Andrew, can you please build a system that's 99% accurate or maybe 99.9% accurate. So one way to establish what might be possible would be to take a data set and look at the Ground Truth Data. Say you have six examples where the Ground Truth Label is these, and then to answer human inspector to label the same data blinded to the Ground Truth Label of course and see what they come up with. And if they come up with these you would say this inspector agreed to the ground truth on four other six examples and disagreed on two out of six. And so Human Level Performance is 66.7%.
Play video starting at :1:28 and follow transcript1:28
And so this would let you go back to the business owner and say look, even your inspector is only 66.7% accuracy. How can you expect me to Get 99% accuracy? So HLP is useful for establishing a baseline in terms of what might be possible. There's one question that is often not asked, which is what exactly is this Ground Truth Label? Because rather than just measuring how well we can do compared to some Ground Truth Label, which was probably written by some other human. Are we really measuring what is possible or are we just measuring how well two different people happen to agree with each other? When the Ground Truth Label is itself determined by a person. There's a very different approach to thinking about Human Level Performance which I want to share of you in this and the next video. Beyond this purpose of estimating Bayes error and establishing what's possible using that to help with their analysis and prioritization. Here are some other users of Human Level Performance. In academia, HLP is often used as a respectable benchmark. And so when you establish that people are only 92% accurate or some of the number on a speech recognition data set. And if you can beat human level performance, then that establishes then that helps you to quote proof that you're learning algorithm is doing something hard and helps get the paper published. I'm not saying this is a great use of HLP, but in academia showing you can beat HLP maybe for the first time has been a tried and true formula for establishing the academic significance of a piece of work and helps with getting something published. We discussed briefly on the last slide what to do if a business of product owner asked for 99% accuracy and if you think that's unrealistic, then measuring HLP may help you to establish a more reasonable target. That's one of the use of HLP that you might hear about. Do not be cautious about which is, I've seen many projects with the machine learning team, wants to use HLP or beating HLP. To prove that the Machine Learning System is superior to the human is doing the job. And as tempting as it is to go to someone and says look, I've proved that my machinery system is more accurate than humans inspecting the phones or the radiologist reading X-rays or something. And now that I've mathematically proved the superiority of my learning album, you have to use it right? I know the logic of that is tempting, but as a practical matter, this approach rarely works. And you also saw last week that businesses need systems that do more than just doing well on average test set accuracy. So if you ever find yourself in this situation, I would urge you to just use this type of logic with caution or maybe even more preferably just don't use these arguments. I've usually found other arguments than this to be more effective that working with the business to see if they should adopt a Machine Learning System. The problem with beating Human Level Performance as proof of machine learning superiority is multi fold. Beyond the fact that most applications require more than just high average tested accuracy, one of the problems with this metric is that it sometimes gives a learning algorithm an unfair advantage when labeling instructions are inconsistent. Let me show you what I mean. If you have inconsistent labeling instructions so that when an audio clip says nearest gas station, let's say 70% of labelers, uses label convention and 30 percent of labelers uses label convention. Neither one is the superiors transcript to the other both seemed completely fine. But just by luck of the draw, 70% of labelers choose the first one, 30% choose the second one. So if the ground truth is established by a labelers, maybe just a laborer with a slightly bigger title, but really by one labelers. Then the chance that two random labeler will agree will be 0.7 squared plus 0.3 squares, which is 0.58. So if you had two labelers use the first convention, there's a 0.7 square chance of that. Or if both of your random labelers use the second convention, there's a 0.3 square chance of that. Then the two of them will agree. So the chances to labelers agreeing 0.58. And in the usual way of measuring Human Level Performance, you will conclude that Human Level Performance is 0.58. But what you're really measuring is the chance of two random labelers agreeing. This is where the machine learning our room has an unfair advantage. I think either of these labeling conventions is completely fine. But the learning algorithm is a little bit better at gathering statistics of how often ellipses versus commas are used in such a context than the learning algorithm may be able to always use the first labeling convention. Because it knows that statistically, it has a 70% chance of getting it right if it uses ellipses or dot dot dot. So a learning algorithm will agree with humans 70% of the time, just by choosing the first lebeling convention. But this 12% improvement in performance, whereas Human Level Performance is 58% and your learning algorithm is 12% better is 0.70. This 12 better performance is not actually important for anything between these two equally good, slightly arbitrary choices. The learning algorithm just consistently picks the first one so it gains what seems like a 12% advantage on this type of query, but it's not actually outperforming any human in any way that a user would care about. And one side effect of this is that, if you're speech recognition tool has multiple types of audio. For some, there's this dot dot dot or ellipses versus common ambiguity and learning album does 12% better on this.
Play video starting at :8:41 and follow transcript8:41
If you're learning algorithm makes some more significant errors on other types of input audio, then when its performance where it actually does worse could be averaged out by queries like these where kind of fake looks like it's doing better. And this will therefore mask or hide the fact that you're learning algorithm is actually creating worse transcripts than humans actually are. And what this means is that a machine learning system can look like it's doing better than HLP. But actually be producing worse transcripts than people because it's just doing better on this type of problem which is not important to do better on while potentially actually doing worse on some other types of input audio. Given these problems with Human Level Performance, what are we supposed to do? Measuring Human Level Performance is useful for establishing a baseline using that to drive error analysis and prioritization. But using it to benchmark machines and humans sometimes runs into problematic cases like this. I found that when my goal is to build a useful application, not publish a paper, you publish a paper, let's prove we can outperform people that helps published paper. But found that when my goal is to build a useful application rather than trying to beat Human Level Performance, I found it's often useful to instead try to raise Human Level Performance because we raise Human Level Performance by improving label consistency and that ultimately results in better learning outcomes performance as well. Let's take a deeper look at this in the next video

# Raisin LP
I think the use of HLP in machine learning has taken off partly because it helped people get papers published to show they can beat HLP. There's also been a bit misused in settings where the goal was to build a valuable application, not just to publish a paper. When the ground truth is externally defined, then there are fewer problems with HLP when the drought really is some real drought-proof. For example, I've done a lot of work on medical imaging, working on your AI for diagnosing from X-rays or things like these. Given an X-ray image, if you want to predict a diagnosis, if the diagnosis is defined according to, say, a biopsy, so your biological or medical tests, then HLP helps you measure how well does a doctor versus a learning algorithm predict the outcome of a biopsy or a biological medical tests. I find that to be really useful. But when the ground truth is defined by a human, maybe even a doctor labeled an X-ray image, then HLP is just measuring how well can one doctor predict another doctor's label versus how well can one learning algorithm predict another doctor's label. That too is useful, but it's different than if you're measuring how well you versus a doctor are predicting some ground truth outcome from a medical biopsy. To summarize, when the ground truth label is externally defined, such as the medical biopsy, then HLP gives an estimate for base error and irreducible error in terms of predicting the outcome of that medical test, the biopsy. But there are also a lot of problems with the ground truth is just another human label. The visual inspection example we had from the previous video showed this, where the inspector had 66.7 percent accuracy. Rather than just aspiring to beat the human inspector, it may be more useful to see why the ground truth, which is just some other inspector compared to this inspector don't agree. For example, if we look at the length of the different scratches that they labeled, say, on these six examples, these were the length of the scratches. If we speak of the inspectors and have them agree that 0.3 mm is the threshold above which a stretch becomes a defect, then what we realize is that for the first example, both label that one totally appropriately. For the second example, the ground truth here is one but is less than 0.3, so we really should change this to zero, then 0.5 guess 1 1, 0.2 000.1. This example has a stretch of 0.1, but really this should have been a zero. If we go through this exercise of getting the ground truth label and this inspector to agree, then we actually just raise human-level performance from 66.7 percent to 100 percent, at least as measured on these six examples. But notice what we've done, by raising HLP to 100 percent we've made it pretty much impossible for learning algorithm to beat HLP, so that seems terrible. You can't tell the business owner anymore, you beat HLP, and thus they must use your system. But the benefit of this is you now have much cleaner, more consistent data, and that ultimately will allow your learning algorithm to do better. When you go is to come up with a learning algorithm that actually generates accurate predictions rather than just proof for some reason that you can beat HLP. I find this approach of working to raise HLP to be more useful. To summarize, when the ground truth label y comes from a human, HLP being quite a bit less than 100 percent may just indicate that the labeling instructions or labeling convention is ambiguous. On the last slide, you saw an example of this in visual inspection. You also see this in speech recognition where the camera versus ellipses..., that type of ambiguous labeling convention will also cause HLP to be less than 100 hundred percent. Improving label consistency will raise human-level performance. This makes it harder, unfortunately for your learning algorithm to beat HLP by the more consistent labels who raise your machine learning album performance, which is ultimately likely to benefit the actual application. Far we've been discussing HLP on unstructured data, but some of these issues apply to structure data as well. You already know that structured data problems are less likely to involve human labors and thus HLP is less frequently use. But there are exceptions. You saw previously the user ID emerging example, where you might have a human label, where the two records belong to the same person. I've worked on projects where we will look at network traffic into a computer to try to figure out if the computer was hacked, and we as human IT experts to provide labels for us. Sometimes it's hard to know if a transaction is fraudulent and you just ask a human to label that. Or is this a spam account or a bot-generated accounts? Or from GPS, what is the mode of transportation is this person on foot, or on a bike, or in the car, or the bus. It turns out buses stop at bus stops, and so you can actually tell if someone is in a bus or in a car based on the GPS trace. For problems like these, it would be quite reasonable to ask a human to label the data, at least on the first pass for a learning algorithm to make such predictions as these. When the ground truth label you're trying to predict comes from one human, the same questions of what does HLP mean? It is a useful baseline to figure out what is possible. But sometimes when measuring HLP, you realize that low HLP stems from inconsistent labels, and working to improve HLP by coming up with a more consistent labeling standard will both raise HLP and give you cleaner data with which to improve your learning experience performance. Here's what I hope you take away from this video. First, HLP is important for problems where human-level performance can provide a useful reference. I do measure it and use it as a reference for what might be possible and to drive air analysis and prioritization. Having said that, when you're measuring HLP, if you find the HLP is much less than 100 percent, also ask yourself if some of the gap between HLP and complete consistency is due to inconsistent labeling instructions. Because if that turns out to be the case, then improving labeling consistency will raise HLP and also give cleaner data for your learning algorithm, which will ultimately result in better machine-learning algorithm performance. Guess what I hope you take away from this video. HLP is useful and important for many applications. For problems where I think how well humans perform is a useful reference, I do measure HLP and I use that to get a sense of what might be possible, and also use HLP to drive error analysis and preservation. Having said that, if, in the process of measuring HLP, you find that HLP is much less than perfect performance, much lower than 100 percent. This is also worth asking yourself, if that gap between HLP and 100 percent accuracy may be due to inconsistent labeling instructions. Because if that's the case, then improving labeling consistency will both raise HLP, but more importantly help you get cleaner and more consistent labels which will improve your learning algorithm's performance

# Obtainin data
You've learned about how to define what should be the data, what should be the definition of y, what should be the definition of the input x. But how do you actually go about obtaining data for your task? Let's take a look at some best practices. One key question I would urge you to think about is how much time should you spend obtaining data? You know that machine learning is a highly iterative process where you need to pick a model, hyperparameters, have a data set, then training to carry out our analysis and go around this loop multiple times to get to a good model. Let's say for the sake of argument that training your model for the first time takes a couple of days, maybe much shorter or maybe longer. Let's say just for the sake of arguments that carrying out error analysis for your project for the first time may take a couple of days. If this is the case, I would urge you not to spend 30 days collecting data, because that will delay by a whole month your getting into this iteration. Instead, I urge you to get in this iteration loop as quickly as possible. Training a model and error analysis might take just a couple of days. I would urge you to ask yourself, what if you were to give yourself only two days to collect data? Would that help get you into this loop much more quickly? Maybe two days is too short, but I've seen far too many teams take too long to collect their initial data set before they train the initial model. Whereas I've rarely come across a team where I said, "Hey, you really should have spent more time collecting data." After you've trained your initial model, carry out error analysis there's plenty of time to go back and collect more data. I found for a lot of projects I've led when I go to the team and say, "Hey everyone, we're going to spend at most seven days collecting data, so what can we do?" I found that posing the question that way often leads to much more creative ways, our ways are still 100 percent respect user privacy and follow regulatory considerations if any, but much more creative, scrappy ways to get a lot of data quickly. That allows you to enter this iteration loop much more quickly and let the project make faster progress. One exception to this guideline is if you have worked on this problem before, and if from experience you know you need at least a certain training set size. Then it might be okay to invest more effort up front to collect that much data. Because I've worked on speech recognition I have a good sense of how much data I'll need to do certain things and I know it's just not worth trying, to train certain models if I have less than certain number of hours of data. But a lot of the time if you're working on a brand new problem and if you are not sure and is often hard to tell even from the literature, but if you're not sure just how much data is needed, then is much better to quickly collect a small amount of data, train a model and then use error analysis to tell you if is worth your while to go out to collect more data. In terms of getting the data you need, one other step I often carry out is to take inventory of possible data sources. Let's continue to use speech recognition as an example. If you were to brainstorm a list of data sources, this is maybe what you might come up with. Maybe I already own 100 hours of transcribed speech data, and because you already own it the cost of that is zero. Or you may be able to use a crowdsourcing platform and pay people to read text. You provide them a piece of text and ask them to read it out loud and just create text data where you already have the transcript because they were reading a piece of text that you have. Or you may decide to take audio that you have that hasn't been labeled yet, and to pay for it to be transcribed. It turns out this is more expensive on a per hour basis than paying people to read texts, but this results in audio that sounds more natural because people aren't reading. For 100 hours of data it may cost $6,000 to get high quality transcripts. Or you may find some commercial organizations that could sell you data. Through an exercise like this, you can brainstorm what are the different types of data you might use as well as their associated costs. One column that's missing from this that I find very important is the time costs, so how long will it take you to execute a project to get these different types of data? For the owned data you could get that instantaneously. For crowdsourced reading you may need to implement a bunch of software, find the right crowdsourcing platform, carry out software integration so you might estimate that that's two weeks of engineering work. Paying for data to be labeled is simpler but still is work to organize and manage, whereas purchasing data maybe there's a purchase order process that may be much quicker. I find it some teams won't go through an inventory process like this and would just pick a random idea, and maybe decide to use crowdsourcing and reading to collect data. But if you can sit down, write to all the different data sources and think through the trade-offs, including costs and time, then that can help you to make often better decisions about what sources of data to use. If you are especially pressed with time, based on this analysis you may decide to use the data you already own and maybe purchase some data and use that over the middle two options in order to get going more quickly. In addition to the amount of data you can acquire and the financial costs and the time costs, other important factors that's application dependent will include data quality. Where you may decide for example, that paying for labels actually gives more natural audio than having people sound like they're reading, as well as really important the privacy and regulatory constraints. If you decide to get data labeled, here are some options you might think through as well. The three most common ways to get data labeled are, in house where you have your own team label the data, versus outsource where you might find some company that labels data and have them do it for you, versus crowdsource where you might use a crowdsourcing platform to have a large group collectively label the data. The difference between outsource versus crowdsource is that, depending on what type of data you have, there may be specialized companies that could help you get the label quite efficiently. Some of the trade offs between these options, having machine learning engineers label data is often expensive. But I find that to get a project going quickly, having machine learning engineers do this just for a few days is usually fine and in fact, this can help build the machine learning engineers intuition about the data. When I'm working on a new project, I often don't mind spending a few hours or maybe a day or two labeling data myself if that helps me to build my intuition about the project. But beyond a certain point you may not want to spend all your time as a machine learning engineer labeling data, and you might want to shift to a more scalable labeling processes. Depending on your application, there may be also different groups or subgroups of individuals that are going to be more qualified to provide the labels, y. If you're working on speech recognition, then maybe almost any reasonably fluent speaker can listen to audio and transcribe it. Speech recognition, because of the number of people that speak a certain language, has a very large pool of potential labels, well, hopefully they will be careful and diligent. For more specialized applications like factory inspection or medical image diagnosis, a typical person off the street probably can't look at a medical X-ray image and diagnose from it, or look at a smartphone and determine what is and what isn't a defect. More specialized tasks like these usually require an SME or subject matter expert, in order to provide accurate labels. Finally there are some applications was very difficult to get anyone to give good labels. Take product recommendations, there are probably product recommendation systems that are giving better recommendations to you than even your best friends or maybe your significant other. For this, you may just have to rely on purchase data by the user as a label rather than get humans to label this. When you're working on an application, figuring out which of these categories of application you're working on and identifying the right type of person or persons to help you label, will be an important step to making sure your labels are high quality. One last tip. Let's say you have 1,000 examples, and you've decided you need a bigger data set. How much bigger should you make your data set? One tip I've given a lot of teams is don't increase your data by more than 10x at a time. If you have a 1,000 examples and you've trained your model on 1,000 examples, maybe it's worth investing to try to increase your dataset to 3,000 examples or maybe at most 10,000 examples. But I would first do a 10x or less than 10x increase first, train another model, carry out error analysis and only then figure out if it's worth increasing it substantially beyond that. Because once you increase your dataset size by 10x, so many things change is really difficult. I found this really hard to predict what will happen when your data set size increases even beyond that. Is also fine to increase your dataset size 10 percent or 50 percent or just 2x at a time, so this is only an upper bound for how much you might invest to increase your dataset size. This guideline hopefully will help teams avoid over investing in tons of data, only to realize that collecting quite that much data wasn't the most useful thing they could have done. I hope the tips in this video will help you to be more efficient in how you go about collecting your data. Now, when you collect your data, one of the things you might run into is the need to build a data pipeline. Where your data doesn't come all at once but there are most complete processing steps that your data has to go through. Let's go on to the next video to take a look at some best practices for building data pipelines.

# Data pipeline
Data pipelines, sometimes also called Data Cascades refers to when your data has multiple steps of processing before getting to the final output. There's some best practices relevant for managing such data pipelines. Let's start with an example. Let's say that given some user information you would like to predict if a given user is looking for a job, because if they're looking for a job at this moment in time, you may want to surface job ads or other pieces of perfectly useful information to them. Given raw data such as the data on top, there's often some pre-processing or data cleaning before the data is fed to learning algorithm that then tries to predict, why? Are they looking for a job? The data cleaning may include things like spam cleanup, such as removing the spam accounts, and maybe also user ID merge which we talked about in an earlier video. For the sake of this example, let's say that spam clean-up and user ID merge are done with just scripting. Explicit sequences of instructions that tells your code when is an account to be considered spammy and when should two user IDs be merged. These systems could be built using machine learning algorithms as well which makes them even a little bit more complex to manage. Now, when you have strips for the data cleaning, one of the issues you run into is replicability when you take these systems into production deployment. Let's say during the development of the system, you have input data fed through pre-processing scripts, and the pre-processed data is fed to a machine learning algorithm, and after some amount of work, your learning algorithm does well on the test set. Printed development phase, you may have seen that pre-processing scripts can be quite messy. It may be you hacking something up, processing data, mailing a file to a different member of your team, having them have a few incantations in Python or some scripting language to process the data and having them, mail the process data back to you. When you take this system to production, you then have new data which has to be fed through a similar set of scripts because this data is going to be fed to the same machine learning algorithm. Your machine-learning algorithm on this data is what will run in your product. The key question is, if you're pre-processing was done with a bunch of strips, spread out on a bunch of different people's computers and laptops, how do you replicate the strips to make sure that the input distribution to a machine learning algorithm was the same for the development data and the production data? I find that the amount of effort that you should invest to make sure that the pre-processing scripts are highly replicable can depend a little bit on the face of the project. I know that it may be fashionable to say that everything you do should be 100 percent replicable, and I'll probably get some criticism for not hewing to that line, but I find it a lot of projects do go through a proof of concept or POC phase, and then a production phase where during the proof of concept phase, the primary goal is just to decide if the application is workable and worth building and deploying. My advice to most teams is during the proof of concept phase focus on getting the prototype to work. It's okay if some of the data pre-processing is manual. If the project succeeds, you need to replicate all of this pre-processing later. My advice would be take extensive notes, write extensive comments to increase the odds that you can replicate all this pre-processing later, but this is also not the time to get bogged down in tons of process just to ensure replicability when the focus is really to just decide if the application is workable and is worth taking to the next phase. Once you decided that this project is worth taking to production, then you know it's going to be really important to do the replica any pre-processing strips. In this phase, that's when I would use more sophisticated tools to make sure the entire data pipeline is replicable. This is when tools which can be a little bit more heavyweight, but tools like TensorFlow Transform, Apache beam, Airflow, and so on become very valuable. In fact, you will learn more about TensorFlow Transform later into specialization as well. In this video, you learned about data pipelines, and when to invest in their replicability. It turns out many applications have significantly more complex data pipelines than what we saw in this video. For those settings, you also have to think about what metadata you want and perhaps also keep track and take care of data provenance and lineage. Let's go on to the next video to look at these topics.

# Meta-data, data provenance and lineage
For some applications, having and tracking metadata, data provenance, and data lineage can be a big help. What do these words even mean? Let's look at an example. Here's a more complex example of a data pipeline, building on our previous example of using user records to predict if someone is looking for a job at a given moment in time. Let's say you start off with a spam dataset. This may include a list of known spam accounts as well as features such as a list of blacklisted IP addresses that spammers are known to use. You might also implement a learning algorithm or a piece of machine learning code and train your learning algorithm, understand dataset, thus giving you an anti-spam model. These arrows indicate flow of information or flow of computation, where training your ML code on the spam dataset gives you your anti-spam model. You then take your user data and apply the anti-spam model to it to get the disband user data. We're following our usual convention that things with the purple rectangle around it represent pieces of code. Now, taking your de-spammed user data. Next, you may want to carry out user ID merge. To do that, you might start off with some ID merged data. This would be labeled data telling you some pairs of accounts that actually correspond to the same person have a machine learning algorithm implementation, train the model on that, and this gives you a learned ID merge model that tells you when to combine two accounts into a single user ID. You take your ID merge model, apply it to the de-spammed user data. This gives you your cleaned up user data. Then finally, based on the clean user data, hopefully, some of this labels with whether someone's looking for a job, you'll then have another machine learning model, train on it to give you a model to predict if a given user is looking for a job or not. This is then used to make predictions on other users or maybe across your whole database of users. This level of complexity of a data pipeline is not atypical in large commercial systems. I've seen data pipelines or data cascades that are even far more complicated than this. One of the challenges of working with data pipelines like this is, what if after running this system for months, you discover that, oops, the IP address blacklists you're using has some mistakes in it. In particular, what if you discover that there was some IP addresses that were incorrectly blacklisted. Maybe because the provider from whom you had purchased a blacklisted IP addresses found out that there were some IP addresses that multiple users use, such as multiple users on a corporate campus or university campus, sharing IP address for security reasons. But the organization creating the blacklist IP address thought it was spammy because so many people shared an IP address. This has happened before. The question is, having built up this big complex system, if you were to update your spam dataset, won't that change your spam model, and therefore that, and therefore that, and therefore that, and therefore that. How do you go back and fix this problem? Especially if each of these systems was developed by different engineers, and you have files spread across the laptops of your machine learning engineering development team. To make sure your system is maintainable, especially when a piece of data upstream ends up needing to be changed, it can be very helpful to keep track of data provenance as well as lineage. Data provenance refers to where the data came from. Who did you purchase the spam IP address from? Lineage refers to the sequence of steps needed to get to the end of the pipeline. At the very least, having extensive documentation could help you reconstruct data provenance and lineage, but to build robust, maintainable systems, not in the proof of concept stage, but in the production stage. There are more sophisticated tools to help you keep track of what happens so that you can change part of the system and hopefully replicate the rest of the data pipeline without too much unnecessary complexity. To be honest, the tools for keeping track of data provenance and lineage are still immature into this machine learning world. I find that extensive documentation can help and some formal tools like TensorFlow Transform can also help but solving this type of problem is still not something that we are great at as a community yet. To make life easier, both for managing data pipelines as well as for error analysis and driving machine learning development, there's one tip I want to share, which is to make extensive use of metadata. Metadata is data about data. For example, in manufacturing visual inspection, the data would be the pictures of phones and the labels but if you have metadata that tells you what time was this picture of a phone taken, what factory was this picture from, what's the line number, what were the camera settings such as camera exposure time and camera aperture, what's the number of the phone you're inspecting, what's the ID of the inspector that provided this label. These are examples of data about your dataset X and Y. This type of metadata can turn out to be really useful because if you discover, during machine learning development, that for some strange reason, line number 17 in factory 2 generates images that produce a lot more errors for some reason. Then this allows you to go back to see what was funny about line 17 and factory 2. But if you had not stored the factory in line number metadata in the first place, then it would have been really difficult to discover this during error analysis. I found many times when I happened to maybe get lucky and store the right metadata, only to discover a month later that that metadata helped generate a key insight that helped the project move forward. My tip is if you have a framework or a set of MLOps tools for storing metadata, that will definitely make life easier but even if you don't, just like you rarely regret commenting your code, I think you will really regret storing metadata that could then turn out to be useful later. Just like if you don't comment your code in a timely way, it's much harder to go back to comment it later. In the same way, if you don't store the metadata in a timely way, it can be much harder to go back to recapture and organize that data. One more example for speech recognition. If you have audio recorded from different brands of smartphones, let's say that in advance, or if you have different labelers labeling your speech, or if you use a voice activity detection model, then just keep track of what was their version number of the voice activity detection model that you use. All of these means that in case for some reason, one version of the VAD, voice activity detection system results in much larger errors, this significantly increases the odds of you discovering that and really use that to improve your learning algorithm performance. To summarize, metadata can be very useful for error analysis and spotting unexpected effects or tags or categories of data that have some unusually poor performance or something else, to suggest how to improve your system. Of course, maybe not surprisingly, this type of metadata is also very useful for keeping track of where the data came from or data provenance. The takeaway from this video is that for large complex machine learning systems that you might need to maintain, keeping track of data provenance and lineage can make your life much easier. As part of building out the systems, consider keeping track of metadata, which can help you with tracking data provenance, but also error analysis. Before we wrap up this section, there's just one more tip I hope to share with you, which is the importance of balanced train dev-test splits. Let's go on to the next video.

# Balanced train/dev/test splits
Many of us are used to taking a data set and randomly splitting it into train dev and tests. It turns out when your data set is small having balanced train dev and test set can significantly improve your machine learning development process. Let's take a look. Let's use our manufacturing visual inspection example. Say your training set has 100 images, so pretty small data set and with 30 positive examples, so 30 defective phones and 70 non defective. If you were to use a train dev test split of 60% of the data in the training set, 20% in the dev or a validation set and 20% in the test set say. Then if you were to use a random split just by chance is not inconceivable that you may end up with 21 positive examples in train, 2 in dev and 7 in tests. This would be quite likely just by random chance. And this means the training set is 35% positive, not that far from 30% positive in the overall dataset, but your dev set is 10% positive and your test set is 35% positive. So 2 out of 20 is 10%, 7 out of 20 is 35%. And this makes your dev set quite non representative because in your dev set you have only 2 or 10% positive examples rather than 30% positive examples. But when your data set is small than all of your 20 dev set examples, it's just a higher chance of this, slightly less representative split. So what we would really want is for the training set to have exactly 18 positive examples, dev set to have exactly 6 positive examples and the test set to have exactly 6 positive examples. And this would be 30%, 30% 30%. And if you could get this type of split, this would be called a balanced split.
Play video starting at :2:37 and follow transcript2:37
Where each of your train, dev and tests has exactly 30% positive examples and this makes your data set more representative of the true data distribution.
Play video starting at :2:50 and follow transcript2:50
There's no need to worry about this effect when you have a large data set. If you have a very large data set, a random split will very likely be representative, meaning that the percentage of positive examples will be quite close to your overall data set. But when you have a small data set with just 20 dev set examples and 20 test set examples, then explicitly making sure you have a balanced split can make your dev set and test set more reliable measures of your learning algorithms performance. This is one of those little techniques that turns out to make a big difference to your performance when you're working on a small data problem, but that you don't really need to worry about if you have a very large data set. So when you have a smaller data set, I hope you consider using a balanced train dev test split as well in terms of how you set up your data set. So when you're working on a smaller data problem I hope that using a balanced change test split will help you with your learning algorithm. And so that's it. Congratulations on getting to this point in this course. You've finished the data section of videos and in the last two weeks you also learned about modeling and deployment. There's just one last optional section that you can watch if you want on scoping. I hope you come with me to watch the optional scoping videos as well. We'll talk about how to select a project to work on. But either way congrats on finishing all the required videos of this course. I hope you learned a lot and that these ideas will be useful for all the machine learning projects.

# What is scoping?
Picking the right project to work on is one of the most rare and valuable schools in Ai today. In the next few videos, I'd like to share with you some best practices for scoping, picking what project to work on and also planning out the scope of the project. I remember when I was younger, I tended to just jump into the first project that I got excited about and sometimes I was lucky and it worked out okay. Now that I've had a little bit more experience, I find that if you buy yourself or your team is going to spend a lot of time, weeks or months or even longer work on the project is well worth your while to think through a few options and try to select the most promising project to work on before putting so much effort into it. So that let's dive into scoping. Let's use the example of an e commerce retailer looking to increase sales. If you were to sit down and brainstorm what and Nikon company could do, you might come up with many ideas, such as maybe a better product recommendation system or better search so people can find what they're looking for or you may find that the catalog data is missing fields or is incomplete and this affects search your recommendations results. So you might see the project to improve the quality of the catalog data. Or you may help them with inventory management, such as deciding how many shirts to buy, where to ship the shirts or what price optimization. With a quick brainstorming session, you may be able to come up with dozens of ideas for how to help this e commerce retailer.
Play video starting at :1:45 and follow transcript1:45
The questions that we like to answer, the scoping process are what project or projects should we work on. Whether the metrics for success and whether the resources such as data time, people needed to execute this project. What I've seen in a lot of businesses is that of all the ideas you can work on. Some are going to be much more valuable than others. Maybe two times or five times or 10 times more valuable than a different idea. And being able to pick the most valuable project will significantly increase the impact of your work. Machine learning is a general purpose to there are a lot of things we could do with machine learning. How do we take valuable projects to work on? Let's dive more deeply into the scoping process in the next video.

# Scopin process
I'd like to share with you a process for scoping projects, that hope will be valuable for how you decide what to work on. When I'm speaking with a company for the first time about their AI projects, this is the process that I use as well. Let's dive in, when brainstorming prices work on. The first thing I do is usually get together with a business or private owner. Often not an Ai person, but someone that understands the business and application and to brainstorm with them. What are their business or application problems? And at this stage I'm trying to identify a business problem, not an Ai problem. So if I'm speaking with Nikon retail business, like the example from the previous video. I might ask, what are the top few things, top three things you wish were working better, and maybe they'll share business problems. Like they like to increase conversions number of people that go to the website and convert to a sale or reduce inventory. So you don't need as much stuff sitting around in the warehouse, or increased margin, increase the profit per item sold. At this point in the process, I'm not trying to identify an AI problem. In fact, I often tell my partners I don't want to hear about your AI problems. I want to hear about your business problems and then it's my job to work with you to see if there is an Ai solution. And sometimes there isn't and that's fine too. Feel free to use the exact same words as well when brainstorming projects with your non AI partners. If you want having identified a few business problems like the three examples on the right. Only then do I see or start to brainstorm if there are possible AI solutions, not all problems can be solved by AI and that's okay. But hopefully we'll come up with some ideas for using machine learning algorithms, to address some of the business problems. I find it is hopeful for this process to separate out the identification of the problem, from the identification of the solution as engineers. We are pretty good at coming up with solutions, but having a clear articulation of what is the problem first often helps us come up with better solutions. This type of separation between problem and solution is something you might hear about in the writings on design thinking as well. After brainstorming a variety of different solutions, I would then assess the feasibility and the value of these different solutions. Sometimes you hear me use the word diligence, to refer to this phrase. Diligence is a term that actually comes from the legal field, but it basically means double-checking if an AI solution really is technically feasible and valuable. Double-checking something that you're hoping, it's true really is true, after validating technical feasibility and value or ROI. Return on investment if you can project if it still looks promising right, if it still looks promising. We then flesh out the milestones for the project and finally budget for resources. Let's take a deeper look at this process of identifying problems, and solutions and we'll use these three examples from Nikon.
Play video starting at :3:35 and follow transcript3:35
So the first one increased conversion, if the business wants to increase conversions, you may have different ideas are doing that. For example, you may want to improve the quality of the website search results. So people find more relevant products when they search. Or you might decide to try to offer a better product recommendations based on their purchase history. It is quite common that one problem may lead to multiple ideas for solutions. And you may be able to bring some other ideas as well, such as maybe a redesign of how products are displayed on the page. Or you may find interesting ways to surface the most relevant product reviews, to help users understand the product and does hopefully purchase it. So there can be many ways to increase conversions. Take the next problem from the previous slide of reducing inventory. Maybe, you can imagine a demand prediction project to better estimate how many people buy something from you. So you don't purchase too many or too few, and have more accurate inventory in your warehouses. Or you may decide to come up with a marketing campaign to drive sales for specifically the products that you bought too many of. So as to steer more purchases of stuff sitting in your warehouse. And that could also reduce inventory, and there could be many other ideas for solutions or for the problem of increasing margin. You may come up with some ways to use machine learning to optimize what to sell. What is worth selling and what is not worth selling. And Nikon retail, sometimes this is called merchandising, just deciding what to sell. Or you can recommend bundles where if someone buys a camera. Maybe you can recommend to them a protective camera case and these bundles can also increase margin.
Play video starting at :5:41 and follow transcript5:41
The problem identification is a step of thinking through whether the things you want to achieve. And solution identification is a process of thinking through how to achieve those objectives. One thing I still see too many teams do today is jump into the first project that they're excited about in my experience. If you have deep domain knowledge about an application or industry may be the first thing your gut gets excited about could be okay. But even then I find it worthwhile to first engage in divergent thinking where you brainstorm a lot of possibilities. To be followed by conversion thinking where you then narrow it down to one or a small handful of the most promising projects to focus on. One thing I hope you might avoid is working really hard on the project and creating a certain amount of monetary or social value. If for the same amount of work, there's a different project that could have created 10 times more, monitoring or social or other positive types of value. And I think this type of scoping process will help you to do that.

# Dilence on fesability and value
In the last video, you heard about the step of assessing a project for technical feasibility and for value. Let's take a deeper look at how you can carry out this diligence step to figure out if a project really is feasible and also how valuable it really is. Let's start with feasibility. Is this project idea technically feasible? Before you've started on the Machine Learning Project, how do you know if this thing can even be built? One way to get a quick sense of feasibility is to use an external benchmark such as the research, literature or other forms of publications, or if different company or even a competitor has managed to build a certain type of online search system before, or recommendation system or, inventory management. But if there's some external benchmark that might help give you a sense that this project may be technically feasible, because someone else has managed to do something similar. Either to complement this type of external benchmark or in the absence of this other external benchmark, here are some other ways to assess feasibility. I'm going to build a two by two matrix that looks at different cases, depending on whether your problem has unstructured data like speech images or structured data like transaction records. On the other axis, I'm going to put new versus existing. Whereby new I mean, you're trying to build a system to do a task for the first time, such as if you've never done demand forecasting before and you're thinking of building one, whereas existing refers to if you already have some existing system, maybe a machine learning one, maybe not, that is carrying out this task and you're thinking of scoping out an improvement to an existing system. New means you are delivering a brand new capability and the existing means you're scoping out the project to improve on an existing capability. In the upper left hand quadrant, to see if a project is technically feasible, I find Human Level Performance, HLP to be very useful and give you an initial sense of whether a project is doable. When evaluating HLP, I would give a human to same data as would be fed to a learning algorithm and just ask, can a human given the same data, perform the tasks such as can the human given a picture of a scratch smartphone, perform the task of detecting scratches reliably? If a human can do it, then that significantly increases the hope they can also get the learning algorithm to do it. For existing projects, I would use HLP as a reference as well. Where if you have a visual [inaudible] inspection system and you're hoping to improve it to a certain level of performance. If humans can achieve the level you're hoping to get to, then that might give you more hope that it is technically feasible. Whereas if you're hoping to increase performance well beyond human level performance, then that suggests the project might be harder or may not be possible. In addition to HLP, I often also use the history of the project as a predictor for future progress. I will say more about both HLP and history of project in the next few slides, but the previous rate of progress on the project can be a reasonable predictor for the future rate of progress on a project. You see more of this later in this video. Moving over to the right column. If you're working on a brand new project with structure data, the question I would ask is, are predictive features available? Do you have reason to think that the data you have, the inputs X are strongly predictive or sufficiently predictive of the target outputs Y? In this box on the lower right, for structured data problem, if you're trying to improve an existing system, one thing that will help a lot is if you can identify new predictive features. Are there features that you aren't yet using but you can identify that could really help predict Y and also by looking at? The history, of the project. On this slide, you heard about three concepts. Human-level performance, the question of whether predictive features are available, and also the history of a project. Let's take a deeper look at these three concepts. Let's start with using HLP on unstructured data images. I use HLP to benchmark what might be durable for unstructured data because people are very good on unstructured data tasks. The key criteria for assessing project feasibility is, can a human given the exact same data as would be given to a learning algorithm, perform the task? Let's look at an example. Let's say you're building a self-driving car, and you want an algorithm to classify whether a traffic light is currently red, yellow, or green. I will take pictures from your self-driving car and ask a person to look at an image like this, and see if the person looking only at the image can tell which lamp is illuminated and in this example, it's pretty clear it's green. But if you find that you also have pictures like these, then I can't tell which lamp is illuminated in this example. This is why it's important for this HLP benchmark to make sure that human is given only the same data as your learning algorithm. It turns out maybe a human sitting in the car and seeing the traffic light with their own eye, could have told you which lamp was illuminated in this example on the right, but that's because the human eye has superior contrast to most digital cameras. But the useful test is not whether the human eye can recognize which lamp is illuminated, the useful test is if the person was sitting back in the office and they can only see the image from the camera, can they still do the task? That gives you a better read on feasibility. Specifically, it helps you make a better guess at whether a learning algorithm, which will only have access to this image, can also accurately detect which lamp in a traffic light is illuminated. Making sure that a human sees only the same data as a learning algorithm will see is really important. I've seen a lot of projects where for a long time a team was working on a computer vision system, say, and they thought they could do it because a human physically inspecting the cell phone or something could detect the defect. But it took a long time to realize that even a human looking only at the image, couldn't figure out what was going on. (If you can realize that earlier,) then you can figure much earlier that with the current camera set up, it just wasn't feasible. The more efficient thing to do would have been to invest early on in a better camera or better lighting setup or something, rather than keep working on a machine learning algorithm on the problem that I think just wasn't durable, with the imaging setup available at a time. Next, for structured data problems, one of the key criteria to assess for technical feasibility is, do we have input features X that seem to be predictive whenever we're trying to predict Y. Let's look at a few examples. In a Ecom example, if you have features that show what are the past purchases of a user and you like to predict future purchases. That seems possible to me because most people's previous purchases are predictive of future purchases. If you have past purchase data, you do have features that seem predictive of future purchases and this project might be worth a try. Or if you work with a physical store, given data on whether if you want to predict shopping mall foot traffic. How many people will go to the mall? While we know that, when it rains a lot, fewer people leave their house, so weather is predictive of foot traffic in shopping malls and so, I will say you do have predictive features. Let's look at some more examples. Given DNA of an individual, let's try to predict if this individual will have heart disease. This one, I don't know, the mapping from your DNA to whether or not you get heart disease is a very noisy mapping. In biology, this is referred to the genotype and phenotype, but the mapping from genotype to phenotype or your genetics to your health condition is a very noisy mapping. I would have mixed feelings about this project, because it turns out your genetic sequence is only slightly, maybe mildly predictive of whether you get heart disease. I'm going to put a question mark there. Given social media chatter, can you predict demand for a clothing style? This is another iffy one. I think you may be able to predict demand for clothing style right now, but given social media chatter, can you predict what will be the hot, fashionable trend six months from now? That actually seems very difficult. One of the ways I've seen AI projects go poorly, is if there's an idea like let's use social media to figure out what people are chatting about in fashion and then we'll manufacture the clothing and sell it in six months. Sometimes the data just is not that predictive, and you end up with a learning algorithm that does barely better than random guessing. That's why looking at whether you have features that you believe are predictive, is an important step of diligence for assessing technical feasibility of a project. One last example that may be even clearer, which is given a history of a particular stock market shares price. Let's try to predict the future price of that stock. All of the evidence I've seen is that this is not doable unless you get some other clever set of features looking at a single shares historical price, to predict the future price of that stock is exceedingly difficult. I would say if those are the only futures you have, those features are not predictive of the future price of that stock based on the evidence I've seen. Even leaving aside the question of how much predicting share prices are trading, if there's any social value, I have some questions about that sometimes. I think this project is also just not technically feasible. Finally, on this diagram, one last criteria I mentioned a couple of times is the history of a project. Let's take a look at that. When I've worked on a machine learning application for many months, I found that the rates of previous improvements can be maybe a surprisingly good predictor for the rate of future improvement. Here's a simple model you can use. Let's see a speech recognition as the example, and let's say that this is human level performance. I'm going to use human level performance as our estimate for B0 or the irreducible level of error that we hope to get to. Let's say that when you started a project, you'll see in the first quarter or Q1 of some year, the system had 10 percent error rate. Over time, in subsequent quarters, the error went down like, Q2, Q3, Q4 and so on. It turns out that it's not a terrible model to estimate this curve. If you want to estimate how well the team could do in the future, one simple model I've used, is to estimate the rate of progress as for every fixed period of time, say every quarter, the team will reduce the error rate by some percentage relative to human level performance. In this case, it looks like this gap between the current level of performance and human level performance is shrinking by maybe 30 percent every quarter, which is why you get this curve that is exponentially decaying to what HRP. By estimating this rate of progress, you may project into the future that hopefully in future quarters, you continue to reduce the error by 30 percent relative to HRP. This will give you a sense of what might be reasonable for the future rate of progress on this project. This gives you a sense of what may be feasible for an existing project for which you already have this type of history and can try to extrapolate into the future. In this video, you saw how to use human level performance, the question of whether you have predictive features and the history of a project in order to assess technical feasibility. Next, let's dive more deeply into assessing the value of a project. We'll do that in the next video.

# Dilience on value
How do you estimate the value of Machine Learning Project? This is sometimes not easy to estimate, but let me share with you a few best practices. Took speech recognition, let's say you're working on building a more accurate speech recognition system for the purpose of voice search so that people speak to the smartphone app to do web searches. It turns out that in most businesses there will be some metrics that machine learning engineers are used to optimizing and some metrics that the owners of the product or the business will want to maximize. There's often a gap between these two building machine learning systems to objective that a learning algorithm may optimize might be something right where level accuracy. If a user says a certain number of words, how many of the words do we get right? Maybe the learning algorithm actually does great in the sense on log-likelihood or some other criteria. But many machine learning teams would be comfortable trying to get good words-level accuracy. But when using this in a business context, one other key metrics query-level accuracy, which is how often do you get all the words in a query right. For some businesses word-level accuracy is important, but query-level accuracy may be even more important. We've now taken one step away from the objective that the learning algorithm is almost directly optimizing. Even after you get the query right, which is important for the user experience, what users care even more about is the search result quality. The reason the business may want to ensure search result quality is that this gives users a better experience and just increases user engagement, so they come back to the search engine more often. This is important, but this is just one step towards actually driving the revenue of the business. One gap I've often seen between machine learning teams and business teams is the entry team will usually want to work on this, whereas the business leader may once promises on that. In order for a project to move forward, I usually try to have the technical and the business teams try to agree on metrics that both are comfortable with. This often takes a little bit of compromise where the machine learning team might stretch a little bit further to the right and the business teams stretch a little bit further to the left. The further we go to the right, the harder it is for a machine learning team to really give a guarantee. I wish more problems could be solved by gradient descent or by optimizing tested accuracy. But that's just not to say of the world today, a lot of practical problems require we do something more than just optimizing tested accuracy. Having the technical team and the business teams both step a little bit outside their comfort zone is often important for reaching compromise to pick some set of metrics that the technical team feels like could stretch a bit to deliver on and that the business team can stretch a bit to feel comfortable will create sufficient value for the business. One other practice I've found useful is that if you can do even very rough back of the envelope calculations to relate what level of accuracy to some of the metrics on the right. If word accuracy improves by one percent, if you have any rough guess for will that improve query level accuracy, maybe by 0.7 percent or 0.8 percent, and how much will that improve search result quality and user engagement and maybe revenue, if able to come up with even a very crude back of the envelope calculation. Sometimes these are also called Fermi estimates. You can read about this on Wikipedia. That can also be a way to help bridge the machine learning and gene metrics and business metrics. Now it goes without saying that as part of estimating the value of a project, I would encourage you to give thought to any ethical considerations as well, such as is this project creating net positive societal value? If not, I hope you won't do it. Or is this project reasonably fair and free from bias and have any ethical or values-based concerns been openly aired and debated? I find it issues of values and ethics is very domain-dependent, is very different in making loans versus health care versus recommending products online. Encourage you to look up any ethical frameworks that have been developed for your industry and your application. If you have any concerns, raise it for debate within your team. Ultimately, if you don't think the project you're working on will help other people or will help humanity move forward, I hope you'll keep searching for other, more meaningful projects to jump into. In my work, I have faced difficult choices where I really wasn't sure if a particular project was something I should work on. Because I really didn't know if it would make people net-net better off, and I found that having a team debate and discuss it openly often helps us get to better answer and feel more comfortable with whatever decision we make. I have called multiple projects on ethical considerations when I felt the project was economically sound, but I didn't think it would help people, and I just told the team, I don't want to do it and I won't do it. I hope this gives you a framework for evaluating the value of a project. On the ethics and value piece, I think all of us collectively should only work on projects that create net positive social value, that help other people that move humanity forward. I personally cool projects just on that basis. There are multiple projects that I felt the economic value was completely sound. The economic case is completely fine, but I look for the price and I said, I don't think this actually helps people and have cool projects just on that basis and told my teams, let's find something else to work on because I'm not going to do that. I hope that you also be able to focus your efforts just on projects that help people and that help move humanity forward.

# Milestones and resourcin
Say you've identified the problem, found a worthy solution, finish diligence for technical feasibility and value and you think this is worth doing. Let's take a look at the last steps of the scoping process of milestones and resourcing. Determining milestones and resourcing involves writing out the key specifications for your project. This will include machine learning metrics such as accuracy or precision-recall. For some applications, this may also include fairness types of metrics. The specification will often also include software metrics regarding the software system such as latency, throughput, queries per second, and so on given the computational resources available, and if possible, you might also write out estimates of the business metrics you hope to move for the project you're scoping, such as how much incremental revenue if you have a way of estimating that? In addition, writing out the resources needed, how much data? From which teams? Personnel, any help you need from cross-functional teams. What software integrations, data leaving support or other support you need from other teams. Finally, the timeline on which you hope to achieve certain milestones or deliverables. Now, if you're looking at a machine learning project and you find that you're having a very hard time writing on some of these key specifications, then you might also consider carrying out a bench-marking exercise to compare it to other similar projects that others may have worked on before, or building a proof of concept first in order to get a better sense of what accuracy, precision, or latency throughput or other metrics might be feasible, and only after you've done that POC, proof of concept, to then use that information to more confidently scope out the milestones and resources needed for a larger scale execution of the project you have in mind. That's it. Congratulations on making it to the end of this section on scoping. I hope that these ideas will help you to pick valuable and meaningful projects to work on. Thank you also for sticking with me from deployment, through modeling, through training, all the way back to scoping, and I hope this framework of the full cycle of machine learning project will also be useful for all the projects I hope you will build and deploy. 