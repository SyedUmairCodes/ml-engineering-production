# Intro to preprocessin
Hello and welcome to feature engineering, transformation and selection. Now, some of this material may seem like it's review, especially from earlier courses or if you've worked in an academic or research setting. But here, we're going to be focusing on production issues, one of which means being able to do all of this at scale in a reproducible and consistent way, so let's get started. Welcome to feature engineering, transformation and selection, we'll start with an introduction to pre-processing.
Play video starting at ::38 and follow transcript0:38
There's a quote from Andrew Ng, coming up with features is difficult, time consuming and requires expert knowledge. Applied machine learning often requires careful engineering of the features and data set.
Play video starting at ::55 and follow transcript0:55
So now let's take a look at what we're going to be talking about, first, we're going to look at how to get the most out of our data and we'll take a look at the art and it really, it is an art of feature engineer. We'll look at the future engineering process itself and how feature engineering is done in a typical ML pipeline.
Play video starting at :1:16 and follow transcript1:16
So let's start with how we're going to squeeze information really out of our data. So, Emma models usually require some data pre processing to improve training and you should probably by now have been training models that this is very familiar to you. What may not be quite so familiar are some of the issues that are involved in production environments, so that's where we'll focus. The way that data is represented can really have a big influence on how a model is able to learn from it. For example, models tend to converge much faster and more reliably when numerical data has been normalized. So techniques for selecting and transforming the input data are key to increase the predictive quality of the models and dimensionality reduction is recommended whenever possible. So that the most relevant information is preserved, while both the representation and prediction ability are enhanced and the required compute resources are reduced. Remember, in production ML compute resources are a key contributor to the cost of running a model, so let's kind of take a conceptual look at feature engineering here. The art of feature engineering tries to improve your model's ability to learn while reducing if possible, the compute resources it requires, it does this by transforming and projecting, eliminating and or combining the features in your raw data to form a new version of your data set. So typically across the ML pipeline, you incorporate the original features often transformed or projected to a new space and or combinations of your features.
Play video starting at :3:20 and follow transcript3:20
Objective function must be properly tuned to make sure your model is heading in the right direction and that is consistent with your feature engineering. You can also update your model by adding new features from the set of data that is available to you unlike many things in ML, this tends to be an iterative process that gradually improves your results as you iterate or you hope it does. You have to monitor that and if it's not improving, maybe back up and take another approach. Feature engineering is usually applied in two fairly different ways, during training, you usually have the entire data set available to you. So you can use global properties of individual features in your feature engineering transformations. For example, you can compute the standard deviation of a feature and then use that to perform normalization. However, when you serve your trained model, you must do the same feature engineering so that you give your model the same types of data that it was trained on. For example, if you created a one hot vector for a categorical feature when you trained, you need to also create an equivalent one hot vector when you serve your model. However, during training and serving, you typically process each request individually, so it's important that you include global properties of your features, such as the standard deviation. If you use it during training include that with the feature engineering that you do when serving, failing to do that right is a very common source of problems in production systems, and often these errors are difficult to find. So to review some key points, as the quote from Andrew Ng demonstrates, feature engineering can be very difficult and time consuming but it is also very important to success. You want to squeeze the most out of your data and you do that using feature engineering, by doing that, you enable your models to learn better. You also want to make sure that you concentrate predictive information, your data into as few features as possible to make the best and least expensive use of your compute resources. And you need to make sure that you apply the same feature engineering during serving as you applied during training.

# Preprocessing Operations
Now let's talk specifically about some of the preprocessing Operations that are used for Feature Engineering. Let me tell you a story. Once when I was first starting out, I was in a hurry and I got the idea, well, I can just skip normalizing my data. I did that. I trained my Model and gosh, it wasn't converging. I tried like adjusting hyperparameters and changing the layers of the Model and looking for issues with the data. It took me a while to remember. Oh yeah, I didn't normalize. Meaning that had an impact. Well, I haven't made that particular mistake again, but, I've made other mistakes instead. Let's hope you don't. First, let's go over what we're going to be talking about. We're going to be going into the main Preprocessing Operations. Obviously this is not going to be an exhaustive list. There's a lot of things you can do to data, but we'll cover some of the main ones anyway. It's going to start, of course, with Mapping raw data into features. Then we're going to look at different features like numerical features and categorical feature. Trying to understand how our knowledge of the data helps guide the way that we transform our features and Engineer better features. Here's some of the main Preprocessing Operation. One of the most important Preprocessing Operations is Data cleansing, which in broad terms consists in eliminating or correcting erroneous data. You'll often need to perform transformations on your data, so scaling or normalizing your numeric values, for example. Since models, especially neural networks, are sensitive to the amplitude or range of numerical features, data preprocessing helps Machine Learning build better predictive Models. Dimensionality reduction involves reducing the number of features by creating lower dimension and more robust data represents. Feature construction can be used to create new features by using several different techniques, which we'll talk about some of them. This is an example of data that we have. We're looking at a house. This is data from a house, but this is only what we start with. The raw data. Feature Engineering because it's in performing an analysis of the raw data and then creating a feature vector from it. For example, integer data can be mapped to floats, and numerical data can be normalized. One-hot vectors can be created from categorical values. Feature Engineering creates features from raw data and you're probably familiar with this. In this example, we're going to take a categorical feature called Street name. We're going to one-hot encoded as a way to make it better for a Model to learn from. We're going to take that string feature, one-hot encoded through Feature Engineering and we get that feature of our Feature Vector through one-hot encoding. But creating a vocabulary is another way to do that. There's a couple of different ways. Tensorflow provides three different functions for creating columns of categorical vocabulary and other frameworks do very similar things. Categorical column with vocabulary lists maps each string to an integer based on an explicit vocabulary lists. Feature name is a string which corresponds to the categorical feature and vocabulary list is an ordered list that defines vocabulary. Feature or categorical column with vocabulary file very similar from a file is used when you have two long list and this function allows you to put the words in a separate file. In this case, vocabulary list is defined as a file that will you define the list of words. This is very typical for working with vocabularies.
Play video starting at :4:36 and follow transcript4:36
But you also know some things about your data and part of that might be domain knowledge of the domain you're working in, or just knowledge of how to work with different data. There's very different operations and preprocessing techniques that can help you increase the predictive information in say, text data. For text, we have things like stemming and lemmatization and normalization techniques like TF-IDF and n-grams, embeddings and that really focuses on the semantic value of the words. That's something we know as data scientists or Machine Learning Engineers about working with data. Images are similar. There's things that we will know about how we can improve the predictive qualities of images. Things like clipping them, resizing them, cropping them, blurring them can often help using filters like Canny filters or Sobel filters or other photometric distortions. All these things can really help us work with image data to improve its predictive quality. Key points. Data preprocessing is a technique that's used to transform raw data into useful data for training the Model. Feature Engineering consists in mapping raw input data and creating a feature vector from it using different techniques with different kinds of data. It can also include things like mapping data from one space into a different space, which depending on the characteristics of a Model, say a linear Model versus a neural network can have a big difference in how well the Model can learn from it.

# Feature Engineering Techniques
Now let's review some Feature Engineering techniques. Here's what we're going to talk about. There's basic feature scaling, usually, especially with numerical data, where you're really doing is scaling the numerical features in different ways. We'll talk about some of the basics of these like normalization and standardization. You can also do bucketizing or binning and we'll talk about that. That's a useful technique as well. Then some other techniques like dimensionality reduction for example. Let's get started. Feature Engineering takes a variety of forms, normalizing, and scaling features, and coding categorical values and so forth. We have scaling and normalizing and standardizing. We can also do groupings, so that could be bucketizing. Where things like for text, we could do a bag of words. This is going to be very dependent on the particular algorithm that you're going to use. You have to understand the connection between the kinds of scaling or grouping that you do and the algorithms that are going to be using with it.
Play video starting at :1:16 and follow transcript1:16
Scaling converts to standard range and different techniques to it differently. For example, a gray-scale image pixel intensity is typically expressed in the raw data as a number between 0-255, and then that's usually re-scale to negative one to one. Here's some Python, for example, for doing that. The benefits of that are, it helps the neural network to converge faster or maybe converge at all. It does away with a lot of the problems with not a number errors during training. For each feature the model is really trying to learn the right way, and having them in the right numerical range, helps a lot with that. Here's the standard formula for normalizing something. It's also called min-max. You're taking your X, you're subtracting the minimum value of that feature. You have to make a full pass over your data set to get that minimum value, and then you subtract that and then you divide by the maximum value minus the minimum value. You're going to need to make a full pass to get those numbers. Then it gives you a number that is between 0-1. Normalizations are usually good if you know that the distribution of your data is not Gaussian. Doesn't always have to be true, but typically that's a good assumption to start with. A good rule of thumb. If you're working with data that you know is not Gaussian, or a normal distribution, then normalization is a good technique to start with. Normalization is widely used for scaling, you have a numerical feature that might start out like this, where numbers are falling and there is a kind of a range. You're going to transform that to the normalized version of that, which is bounded in a range between 0-1. That's normalization. You're probably familiar with this although you may not have thought of some of the production implications like the fact that you need to make a full past career data set in order to get the numbers that you're going to need to perform normalization. The scales here are again, between 0-1 and whatever the original was. Standardization, which is often using a Z-score, is a different technique. It's a way of scaling using the standard deviation. It's looking at the distribution itself and trying to scale relative to that. The example here is using, , and you're going to subtract the mean and divide by the standard deviation. That gives you the Z- score or the standardized value of X. Which is somewhere between zero and the standard deviation. This is how that's expressed, actually between some multiple of the standard deviation. But it's centered around the mean of the data. If the original looked like this, again, a standardized value of that might look like this. Notice that this score is centered on zero, so the mean is translated to zero. But you can have negative values and positive values that are beyond one. It's a little bit less bounded transformation than a normalization is. But there are some advantages to it, that your data is a normal distribution, then a standardization technique is a good place to start, it's a good rule of thumb to start with for your numerical features. But I encourage you to try both standardization and normalization and compare the results. Because sometimes it doesn't make much of a difference. Sometimes it can make a substantial difference. It's good to try both. Moving on into bucketizing and binning. We're going to take a look at an example where we have a house that was built in a particular year and we're going to bucketize that. Often you don't want to enter a number directly into a model. Instead, you want to encode it as a category by grouping it into buckets. You notice how we've taken our number, our year, and we've created a one-hot encoding of that that helps us learn from that number in a more really relevant way because the difference between 1960 and 61 isn't nearly as important as the difference between 1960 and 1970 in terms of the value that this feature contained. Looking at how it gets binned into one-hot encoded vector, you can see it's put it into different buckets. We can also look at that using facets, which is a nice visualization tool to help us look at that. This really demonstrates some of the value of bucketizing and binning, but it also is a good way to look at your data and try to understand it. Which is an important part anytime you're working with data, is making sure that you really have a good solid understanding of what's happening in your data and develop an intuitive sense for it. This kind of visualization can really help you do that. Some other techniques. There's dimensionality reduction techniques, for example, that you can do. There's PCA, which is going to project your data along the principal components in order to reduce the dimensionality of it. There's t-SNE, which is an important technique for embeddings, often very useful. Uniform manifold approximation and projection is a less well-known technique, but interesting and has some interesting aspects. We won't really go into that in detail. Then feature crossing as well. One of the things that you can use when you're working with embeddings is an embedding projector, like the TensorFlow embedding projector that we're looking at here. You can quickly get an idea of what your data looks like at a particular space. That again, is helping you to develop that intuitive sense of your data. You can already see just looking at it, you can get a feel for how it's clustered. You can see where different types of data land in that space. That understanding is very important to help you work with your data. This is really where some of the art of feature engineering comes into play, where you as a developer developed this understanding of your data. As an intuitive exploration, really important for high-dimensional data because we as humans can visualize maybe three dimensions and then it gets really weird. Even four is hard and 20 is impossible. Visualizing it and analyzing it is important. There's different techniques for doing that or for doing dimensionality reduction along that to help you understand that. PCA that you're probably familiar with principal component analysis and t-SNE and UMAP that we just talked about a little bit. Then there's other custom linear projections as well. That TensorFlow embedding projector is available. It's free, you can go play with it. It's actually a lot of fun, but it's also a great tool to really help you understand your data. There's the address for it if you want to learn more. Key points for this particular section, feature engineering. It's going to prepare and tune and transform and extract and construct features where we're going to work with features and change them starting with our raw data through to the data that we're going to give to our model. Feature engineering is very important for model refinement. Really, it can make the difference between successfully modeling something and not. Feature engineering really helps with ML Analysis and really developing that intuitive understanding of our data.

# Feature crosses
Now let's take a look at Feature Crosses. Feature Crosses is the main thing that we're going to be talking about in this section. We're also going to take a quick look at encoding features. What are Feature crosses? Well, they combine multiple features together into a new feature. That's fundamentally what a feature across. It encodes non-linearity in the feature space, or encodes the same information and fewer features. We can create many different kinds of feature crosses and it really depends on our data. It requires a little bit of imagination to look for ways to try to do that, to combine the features that we have. For example, if we have numerical features, we could multiply two features and produce one feature that has an expression of those two features. X number of features, we could multiply them all together if there are numerical features and then end up with one feature instead of say five. That's great. We can also take categorical features or even numerical features and combine them in ways that make a semantic sense. Taking the day of the week and hour, if those are two different features that we have, putting those together, we can express that as the hour of the week and we have the same information in one feature. To encode features, well, let's look at some examples. Suppose we have a data-set that's looking at healthy trees and sick tree. We're going to use a scatter-plot to try to understand our data. We're looking at the scatter plot and we use some visualization tools, and we ask ourselves, can we draw a line to separate these two groups, these two clusters? If we can use a line to create the decision boundary, then we know that we can use a linear model. That's great. In this case, looking at this, we could use a line to do that separation. That's great. Linear models are very efficient. We love that, but let's suppose the data looks like this. Not so easy anymore. This becomes a non-linear program. Then the question is, can we project this data into a linear space and use it with a linear model, or do we have to use a non-linear model to work with this data? Because if we try to draw just a linear classification boundary with a data as is, it doesn't really work too great. Again, we can use visualization tools to help us understand this. Looking at our data really helps inform us and guide the choices that we make as developers about what kind of models we choose and what feature engineering we apply. Key points here, Feature Crossing as a way to create synthetic features, often encoding non-linearity in the features space. We're going to transform both categorical and numerical. We could do that in, into either continuous variables or the other way around.

# Preprocessing Data at Scale
We've probably all done feature engineering before, but it's one thing to do it in a notebook with maybe a few 100 megabytes of data. And quite another thing to do it in a production environment with maybe a couple of terabytes of data in a repeatable automated process. Let's take a look now at how to do feature engineering at scale, okay? So it's one thing to do transformations and feature engineering on a one off basis saying a notebook. But it's another thing to do it at scale in a production environment in a reproducible and repeatable and consistent way. So let's take a look now at pre processing data at scale. So first let me tell you a story. I once worked as a data scientist in a production environment where we were deploying our workloads are production workloads on Apache Storm and it it was a nice platform. The performance was was pretty good on it. We as data scientists created our models in notebooks and then deployed them into Apache Storm. Well it wasn't quite that simple. We were developing our notebooks in python and then when we deployed into storm we had to translate all of the feature engineering that we did into java. Well, as you can imagine, that was not ideal and there were little weird problems that cropped up often very difficult to find doing that transformation. This is not an ideal way to do production machine learning. So what is much better technique is to use a pipeline, a unified framework where you can both train and deploy with consistent and reproducible results. So we'll be learning about that in this course. Let's take a look now at some of the issues of doing feature engineering at scale. So here's what we're going to be talking about inconsistencies in feature engineering very important. Preprocessing granularity. Also an interesting issue, working in production environment. Preprocessing the training data set is one thing and optimizing the instance level transformations for that is, something kind of different. But there's a number of challenges that you need to deal with. So to preprocessed data at scale, we start with real world models and these could be terabytes of data. So when you're doing this kind of kind of work, you want to start with a subset of your data work out. As many issues as you can think about how that's going to scale up to the terabytes that you you need to actually work with your whole dataset. Large scale data processing frameworks are no different than what you're going to use, on your laptop or in a notebook or what have you. So you need to start thinking about that from the beginning of the process, how this is going to be applied and the earlier you can start working with those frameworks. The more you work out the issues early with smaller datasets and quicker turnaround time, consistent transformations between training and serving are incredibly important. If you do different transformations when you're serving your model than you did when you were training, your model, you are going to have problems and often those problems are very hard to find. So inconsistencies and feature engineering the training and serving code paths. When you are training your model, you have code that you're using for training. If you're using different code, like we were we were using python for training and java for serving, that's a potential source of problems. So, and you could have different deployment scenarios. You you might be deploying your model for in different environments. You you could be using the same model, say, in a servant cluster and using it on an IOT device as well. So there's different deployment targets and you need to think about those and what are the CPU resources or the compute resources that you have available on those targets. So mobile, for example, very tight compute resources, servers, you have more luxury, but again, cost is a big factor and in a web browser, that could be deployed on different clients. So you need to think about that as well. You don't want to be too heavy. Though, there's risks in introducing training, serving skews. So that's what happens because of those different code paths between training and serving. If you don't have exactly the same transformation is happening between the two and the best way to do that is used exactly the same code. Then you have a potential problem and that's going to lower the performance of your model. So your model may, completely just error out or give wacky results or it may be just slightly off in certain circumstances and display sensitivity around certain things. Those things are much harder to detect. So there is a notion of the granularity at which you're doing preprocessing. So you need to think about this, you're going to do transformations, both the instance level and a full pass over your data. And depending on the transformation that you're doing, you may be doing it in one or the other. You can usually always do instance level. But full path requires that you have the whole dataset. So for clipping, even for clipping, you need to set clipping boundaries. If you're not doing that through some arbitrary setting of those boundaries, if you're using the data set itself to determine how to clip, then you're going to need to do a full pass. So min max for clipping is is going to be important. Doing a multiplication at the instance level is fine, but scaling well now I'm going to need probably the standard deviation and maybe the min and max. Bucketizing similarly, unless I know ahead of time what buckets are going to be, I'm going to need to do a full pass to find what buckets makes sense. Expanding features I can probably do that at the instance level. So these two things are different at training time. I have the whole dataset so I can do a full pass, although it can be expensive to do that. At serving time, I get individual examples, so I can only really do instance level. So anything I need to do that requires characteristics of the whole dataset. I need to have that saved off so I can use it at serving time.
Play video starting at :7:21 and follow transcript7:21
So when do you transform, you're going to pre-process your training dataset and there's pros and cons in how you do that. First of all, you you don't you only run once per training session, so that's cool. And you're going to compute it on the entire dataset, but but only once, for each training session. The Collins well, you're going to have to reproduce all those transformations that serving or save off the information that you learned about the data, like the standard deviation. So that you can use that later while you're serving. And there's, slower iterations around this. Each each time you make a change, you've got to make a full pass over the dataset. So you can do this within the model. Transforming within the model has some nice features to there are cons as well. First of all it makes iteration somewhat easier because it's embedded as part of your model and there's guarantees around the transformation that you're doing. But the cons are it can be expensive to do those transforms, especially when, your compute resources are limited. And think about when you do a transform within the model, you're going to do that same transform at serving time. So you may have say GPUs or TPUs when you're training, you may not when you're serving. So there's long model latency, that's when you're serving your model, if you're doing a lot of transformation with it that can slow down the response time for your model and increase latency. And, you can have transformations per batch that that skew that we talked about. If you haven't saved those constants that you need, that could be an issue. You can also transform per batch instead of for the entire dataset. So you could for example, normalize features by their average within the batch. That only requires you to make a pass over each batch and not the full data set, which when you're working with terabytes of data. That can be a significant advantage in terms of processing. And there's ways to normalize per batch as well. So you can compute an average and use that and normalization per batch and then do it again for the next batch there will be differences batch to batch. Sometimes that's good in cases. So for example where you have changes over time in a time series, that can be a good thing but you need to consider that. So normalizing by the average per batch, precompeting the average and using it during normalization, you can use it for multiple batches for example. So this is different ways to try to think about how to work with your data when you, especially when you have a large dataset. You need to think about optimizing the instance level transformations as well because this is going to affect both the training efficiency and you're serving efficiency. So you're going to have accelerators that you need to consider. They may be sitting idle while your CPU is doing transforms. That's something that you want to try to avoid because accelerators tend to be expensive. So as a solution you can prefetch your your transformations and use your accelerators more efficiently. So by prefetching you can prefetch with your CPU feed your your accelerator your GPU or CPU and try to paralyze that processing. So again this all gets down to cost and how much it costs to train and and the time required as well to train your model and how efficient it is. So to summarize the challenges we have to balance the predictive performance of our model and the requirements for it. Making folks passed transformations on the training data is one thing. If we're going to do that then we need to think about how that works when we serve our model as well and save those constants. And we want to optimize the instance level transformations for better training efficiency. So things like prefetching can really help with that. So key points, inconsistent data affects the accuracy of the results. So scalable data processing frameworks allows to process large datasets and distributed inefficient ways. But we also need to think about how that gets applied in serving.

# tensorflow transform
To do pre-processing at scale, we need good tools. TensorFlow Transform is one of the best tools available today. Let's take a look. First of all, here's what we're going to be talking about. We are going to go a bit deeper into how transform works, what it does, and why it does it. We are going to look at the benefits of using TensorFlow Transform and how it applies feature transformations, and we are going to look at some of the analyzers, and the role they play in doing pre-processing. Let's talk about transform. First off, it does what you might think it would do, it takes Training Data, and it's going to process it. The end result is going to be deployed into the Serving System. In the middle, there's a PIPELINE that is used, and there is METADATA that forms a key role in organizing and managing the artifacts that are produced as data is transformed. One of the reasons that's important, is because, we want to understand the lineage or provenance of those artifacts, and be able to connect them, and find the chain of operations that generated each of them. We have our Training Data, that is the input data into the system that forms an artifact, we give that to Transform and it transforms our data. It's going to take our raw data and move it into the form that we're actually going to use at our feature engineering. That's given to the Trainer which is going to do training. Transform and Trainer here are both components in a ML pipeline, specifically a TFX ML pipeline. These are the Trainer, of course as a Trained Model, that is also an artifact. That gets delivered to the serving system or whatever system we're using, where it's going to be used to run inference. Looking at this a little differently, we're starting with our training and eval data. We've actually split our dataset. We split it with ExampleGen, so ExampleGen does that split. Those get fed to StatisticsGen. These are both TFX components within a TFX pipeline, so ExampleGen is component, StatisticsGen is a component. StatisticsGen calculates statistics for our data. For each of the features, if they're numeric features, for example, what is the mean of that feature value? What is the standard deviation? The min, the max, so forth. Those statistics get fed to SchemaGen which infers the types of each of our features. That creates a schema that is then used by downstream components including Example Validator, which takes those statistics and that schema, and it looks for problems in our data. We won't go into great detail here about the things that it looks for, but if we have examples that are the wrong type in a particular feature, so maybe we have an integer where we expected a float. Now, we're getting into transform. Transform is going to take the schema that was generated on the original split dataset, and it's going to do our feature engineering. Transform is where the feature engineering happen. That gets given the Trainer, there's Evaluator that evaluates the results, a set of Pusher that pushes it to our deployment targets which is, however we're serving our models, so TENSORFLOW SERVING, or JS, or LITE, wherever it is that we're serving our model. Internally, if we want to look at the transform component, it's getting inputs from, as we saw, ExampleGen and SchemaGen. That is the data that was originally split by Example Gen, and the schema that was generated by SchemaGen. That schema, by the way, may very well have been reviewed and improved by a developer who knew more about what to expect from the data than can be really inferred by SchemaGen. That's referred to as curating the schema. Transform gets that and it also gets a lot of user code because we need to express the feature engineering we want to do. If we're going to normalize a feature, we need to give it user code to tell transform to do that. The result is a TensorFlow graph, which were referred to as the transform graph and the transform data itself. The graph expresses all of the transformations that we are doing on our data, has a TensorFlow graph. The transform data is simply the result of doing all those transformations. Those are given to trainer, which is going to use the transformed data for training and it's going to include the transform graph and we'll take a look at that in a second. We have a user provided transform component and we have a schema for it. We apply our transformations during training and as we'll see later, we also apply those transformations at serving time. There's a lot of perform optimizations that we apply as well, and we'll take a look at that in a second. Continuing a little deeper here, we have our raw data and we have our transform component. It produces a TensorFlow graph. The result of running that graph is processed data, it's been transformed. That gets given in model training, as processed data is given to our trainer component, where we use it for training our model. Training our model creates a TensorFlow graph. Notice now we have two different graphs. We have a graph here from transform and a graph here from training. Using the Tf Transform API, we expressed the feature engineering that we want to do and we give that code, or rather the transform component gives that code to a Apache Beam distributed processing cluster. That way we can do, and remember this is all designed to work with potentially terabytes of data. That could be quite a bit of processing to do all of that transformation using a distributed processing cluster by using Apache Beam gives us the capacity to do that. The result is a saved model. Saved model is just a format that expresses a trained model as saved model. That gets included, we have both the transform graph and the training graph. Those are given to our serving infrastructure. Now we have both of those graphs, the feature engineering that we did, and the model itself, the trained model and those are used when we serve requests. Looking at this a little differently, we have analyzers.
Play video starting at :7:45 and follow transcript7:45
Again, we're using TensorFlow and we're using the TensorFlow Transform API. We're using TensorFlow Ops and a big part of what they do, what an analyzer does is it makes a full pass over our dataset in order to collect constants that we're going to need when we do feature engineering. For example, if we're going to do a min-max, well, we need to make a full pass through our data to know what the min and the max are for each feature that we're using for that. Those are constants that we need to express. We're going to use an analyzer to make that pass over the data and collect those constants. It's also going to express the operations that we're going to do. They behave like TensorFlow Ops, but they only were on once during training and then they're saved off as a graph. For example, if we're using the tft.min, which is one of the methods in the transform STK, it'll compute the minimum of a tensor over the training dataset.
Play video starting at :9: and follow transcript9:00
Looking at how that gets applied, we have the graph that is our feature engineering. We run an analysis across our dataset to collect the constants that we need. Really what we're doing is we're enabling us later to apply these into our Transform graph, so that we can transform individual examples without making a pass over our entire dataset. That gets applied during training, and the same exact graph gets applied during serving. There is no potential here for training and serving skew or having different code paths when we train our model versus when we serve our model that cause problems and those not be equivalent. We're using exactly the same graph in both places, so there is no potential for doing that. What is the benefits of that? Well, the emitted graph that Transform emits is it has all necessary constants and transformations that we're going to need. The focus is on data pre-processing only at training time. It's doing that processing as well as generating that graph. Have works in-line during both training and serving, we prepend it to our trained model. There's really no need for pre-processing code at serving time. It consistently applies those transformations irrespective of the deployment platform. Remember, we could be deploying our model to a TensorFlow server or a mobile device using TensorFlow Lite or a web browser using TensorFlow Jazz. Using the same TensorFlow graph in all places, we're going to get exactly the same transformations. The Analyzers framework itself has some different aspects. First of all, you can do scaling. Things like scaling with a z-score or just scaling between zero and one, bucketizing. We're going to bucketize the quantiles, for example. We're going to set up a set of buckets and then we're going to take those values that we have and assign them to the bucket so that we have categorical values for numerical ranges of value or vocabularies. Things like running [inaudible] for example, bag of words or n-grams. Intact processing need to work with vocabulary. You could do dimensionality reduction too. You can do a PCA Transform to reduce the dimensionality of your data. Now let's look at some code. We're going to create a pre-processing function. This is the entry point that we're going to use to define the user code that expresses the feature engineering they're going to do. For example, we may make a pass over our data to look for floats. For our floats, we're going to scale those using a z-score. This is just an example. You're going to do whatever feature engineering you have to do, but it's this style of code that you're working with, and this is of course, Python. A vocabulary very similar. If we have vocabulary features or that we want to use with vocabulary, we would do this thing. These constants here are lists of the keys for each of the features that we want a grip into that particular transformation. Bucket features, exactly the same thing. That bucket feature key is constant, is just a list of the keys for the features that we want to bucketize. Imports, well, you're going to import TensorFlow, obviously. Apache Beam, you're going to need that because remember, Transform is going to use Apache Beam to distribute processing across a cluster. Now you can also just use Beam on a single system or you can just run it on your laptop. It has something called the DirectRunner. You don't need to actually set up a cluster. You can just run it, and there's nothing to move infrastructure to setup. In development that's pretty useful. In a deployment, unless you're working with a small amount of data, you probably want more compute horsepower than that, apache_beam.io.iobase is part of that. It helps us with IO in different formats and different ways reading and writing. TensorFlow Transform itself, obviously. Often we use the abbreviation of TFT for that. Then Transform itself has a Beam part of its modules. It's often nice to pull out separately and work with. Those are usually the basic imports that we need to work with Transform.

# Hello World with tf.Transform
Now let's put this all together and look at a Hello world example with tensorflow Transform. So here's what we're going to do. We're going to collect some raw data and we're going to define the metadata around that data. Then we're going to run transform to transform the that raw data into features and that's going to produce a graph and we're going to in the background, we're going to be running analyzers and using tf.Transform. So we start with collecting our raw data which in this case is very simple, it's just some static data that we've created for the example. So we have three different features here, x, y and s. So we express the types of those features and information about them as metadata. So we're going to import the metadata module with intensive transform and we're going to express that metadata using a feature spe. So we can create a feature spec that expresses some information about our our feature. So this is telling us that, why is afloat feature first of all, and then it's a fixed length feature. So this is not a for example a sparse feature or ragged tensor, it's fixed length feature. So both y and x, or fixed length float features and then s, is a string feature also fixed length. So now we go this is our preprocessing function. This is where our user code is going to go. It's the entry point for our user code anyway, so you can see here we're pulling the values for x y and s from our inputs and then we're going to do some transformations. These are very simple transformation. So going to center x and that's going to require the mean. So that in the background without us thinking about it really is going to make a pass over the data or as it makes the pass, it's going to collect that means so we can use it here. We're also going to do a normalization using very simple scale to between zero and one. So that's our y normalize again going to need a pass over the data to do that. That's going to be the same pass, it only does one pass. And then we're going to take that string feature that we have s and we're going to create a vocabulary for it and apply it so that we get an integer value for for s, a vocabulary. And then we create a feature cross, so this is purely synthetic feature. So we're using our value for x, that we've centered in our value for y that we've normalized. And we're creating a synthetic feature called appropriately x centered times y normalized which is verbose but accurate. Okay, and then that's going to return well, the values that we just created so that's our feature engineering. We took in raw data and here are transformed features that we've created. We've engineered these features. So again we took tensors in, so for x we took integer values and for s we took strings and for y we also took integer values. Those passed through the code that we wrote to express our feature engineering. The entry point of that was the pre processing function. And coming out of that we get our engineered features including the vocabulary version of our string feature that we have created vocabulary for. So now we're going to run our code to do the processing that requires us to create a main in this case because we're just using pure tensorflow transform as a library to run our feature engineering. I want to point out though typically you would often be doing this in a TFX pipeline using a transform component. The code is really the same in the preprocessing function that we just looked at. But here it's a little bit different because we're running in tensorflow transform and not in a transform component in a TFX pipeline. So this is one of the ways that you can run transform. I just want to make you aware that this part that we're looking at here is not how it looks, when you run it in TFX. So here we're using we're just running it inside a main function and we're going to do that using Apache beam. That's going to require us to establish a context for Apache beam. And then we're going to define a beam pipeline using the beam python syntax, which is a little bit different than you might expect. One of the things to get used to here is the pipe operator. So what that says is I'm going to run tft beam. I've established that in the context, I'm going to use the analyze and transform data set to run my preprocessing function. That's going to return a result actually in this case is a tuple result. So it's going to return both the raw data and the raw data metadata. So this python syntax is a little bit different than what you might be used to for python that is specific to Apache be. So we have our transformed data, we have our transform metadata and we get that from our transformed data set. So we can print out the raw data and the transform data and run our main so before we ran transform we had features like this. After running transform we have features like this.
Play video starting at :6:21 and follow transcript6:21
So that's our transformation, we've we've transformed between our raw data and our engineer data. Yeah so, key points on this Hello world example of using tensorflow transform actually outside of tfx, although you would often be running this as part of the tfx transform component, it allows preprocessing on the input data and creating the features. Well, that's what transform is for. It's the feature engineering that we're going to do. It allows defining preprocessing pipelines that are going to run and and do their execution on large scale data processing frameworks using Apache beam. Apache beam runs on top of other frameworks like spark or flank or google cloud data flow or actually just on your laptop, using what's called the direct runner. In a tfx pipeline the transform component implements feature engineering using tensorflow transform. So the transform component is built using the transformed library, the tensorflow transformed library.

# Feature spaces
What are the important aspects of shaping your data is to consider the feature space that your data covers? Let's take a look. So here's what we're going to be talking about. First of all just an introduction to feature spaces and then an introduction to feature selection as well. We'll talk about filter methods and wrapper methods and embedded methods. Those are methods of feature selection. In this first section, we're really just looking at feature spaces.
Play video starting at ::33 and follow transcript0:33
So what are feature spaces? Well, a feature space is defined by the n dimensional space that your features defined. So if you have two features, a feature space is two dimensional. If you have three features, its three dimensional and so forth, it does not include the target label. So that we're just talking about your features. So if this is your feature factor, you have future vector X. And it has values from zero to D. Here. That's the dimensionality of that vector. And that defines a space. So in this case that's a three D space. We have three features. We have a 3D. Space, or if we're looking at it in a 2D space with with a two dimensional feature vector, we could express it as a scatter plot with a 2D space. Those are fairly easy for us as humans to imagine.
Play video starting at :1:30 and follow transcript1:30
So looking at it a little differently. Here's an example using features that you might have for a house, so the number of rooms for the house or actually any kind of building really the square footage of it where it's located in a price. So maybe we're trying to figure out how sales.
Play video starting at :1:51 and follow transcript1:51
So in this case, the F is your model and it's acting in your feature space and it's going to produce a result Y, looking at a classification situation or problem we have again. So this is going to be a two dimensional space. We have different distributions of the examples within our feature space. In an ideal case, they're easy separable, ideally with a linear function in a realistic case. Well, maybe we can do that with a linear function if we sort of fudge around some of the some of the examples and then we have, poor examples where it's difficult to do with a linear function. We're working in a non linear feature space and and, maybe we can use a projection to help with that. Or maybe we just have to use a non-linear model, but we're, what we're going to try to do is draw a boundary within that feature space. So, here's a case where we're drawing, maybe a parametric boundary. But the model is going to try to learn that decision boundary.
Play video starting at :3:5 and follow transcript3:05
The boundary is used to classify the data points. So that's what our model is learning as a result of knowing where that boundary is. It can take an example that is given to it and decide in this case which class it falls into.
Play video starting at :3:22 and follow transcript3:22
So feature space coverage is important. The examples that you give to your data to your model rather to train and for evaluation have to be representative of the examples that you're going to see when you serve your model. In other words, the requests that are going to be given to your model and where they fall in your feature space, that region of your feature space has to be covered when you train and evaluate your model as well. So that means the same numerical ranges were for classes the same classes and you need similar characteristics for image data as well. So it's not quite so, numerical but it's still expressing a space and similarly for vocabulary. In this case we have syntax and semantics that we have to consider as well for natural language problems.
Play video starting at :4:19 and follow transcript4:19
So we need to ensure that we covered the same space. If we're doing time series problem, we need to consider seasonality trend and drift. And that includes for the serving data, we may have new values that we find in new labels that's going to influence the concept drift as as we've talked about previously, so we need to be aware of that and design processes to deal with that.
Play video starting at :4:49 and follow transcript4:49
That means we need continuous monitoring, which is going to be a key for success in these situations. Remember as we've talked about before, the world changes and our model only learns one version of the world. So when the world has changed, we need to update our model to the new world.

# Feature selection
Now, let's talk about feature selection. What is feature selection? Well, we have a number of features and as it turns out, we may need some of them and perhaps not all of them.
Play video starting at ::14 and follow transcript0:14
So we try to select which features we actually, need and eliminate the features that we don't need. So that gives us a smaller set of features, which are useful features. The ones that we have selected. Feature selection identifies the features that best represent the relationship between the features, and the target that we're trying to predict. So, it removes features that don't influence the outcome.
Play video starting at ::44 and follow transcript0:44
That reduces the size of the feature space. So remember, depending on the number of features in the feature vector, that defines the size of a space. Every time we add a feature, the space increases exponentially. So we want to try to reduce the number of features. That reduces the resource requirements for processing our data, and the model complexity as well. So why is feature selection needed? Well, we want to reduce storage and I/O requirements, that's part of it. Less features means smaller data. And we want to minimize the training, and inference costs. Especially in many cases the inference costs, because we may be serving millions of requests with once we have trained our model, and each one of those costs a certain amount to serve. So, there's different ways to do feature selection. First of all, you can do unsupervised feature selection, meaning you don't have to or doesn't account really for the labels, or supervised feature selection. So, it is going to use the information in the labels. So for unsupervised feature selection it takes the features, and the target variable relationship is not considered. It removes the redundant features which in unsupervised, really means looking for correlation. When you have two features or more than two features that are highly correlated, you really only need one of those, and you're going to select the one that probably gives you the best result.
Play video starting at :2:24 and follow transcript2:24
For supervised feature selection, it is going to use the target relationship. So the relationship between each of the features, and the target or label. So it's going to select those features, that contribute most to correctly predicting the target. So let's take a look at some supervised methods. We're going to use feature selection, and there's different methods to do that filter methods, wrapper methods, and embedded methods. For a practical example, we're going to work with a dataset from breast cancer diagnostic. So this is going to be the data set we're going to use for looking at several different kinds of feature selection, but this is going to be our example. So we're going to try to predict whether the tumor is benign or malignant, and that gives you some notion of how cancer reproduces. This is our feature list, and as you can see we've created and assigned an id for our example. And we also have an irrelevant feature that's been added here, so that we have something there. This is just an example, usually it's not quite so obvious as something named unnamed 32, but you do often see not a number of values. So that's often a clue or missing values, that's often a clue that you have an irrelevant feature.
Play video starting at :3:50 and follow transcript3:50
And for performance evaluation, will start as a baseline value. We're going to use a random forest classifier, using sklearn to work with our selected features. And we're going to use some metrics for that. So that's going to include the accuracy score, the AUC score, the precision, recall, and F1.
Play video starting at :4:17 and follow transcript4:17
So, this is our baseline here with all 30 features. It gives us for each of those metrics, the values that we get for all 30 features

# Fitler metods
Let's start with filter methods. Filter methods are one of the supervised methods of doing feature selection, which includes wrapper methods and embedded methods also. But for filter methods, we're primarily using correlation to look for the features that contain the information that we're going to use to predict our target. Univariate feature selection is also very often used for efficiency really. Let's take a look at some filter methods. Correlated features are usually redundant as we just talked about. When you have to features that are highly correlated with each other, that usually is an indicator that they're redundant. You want to choose one of those so you remove the other one. Popular filter methods include Pearson correlation. These are different ways to do correlation. Pearson correlation is one. That's correlation between features and between the features and the label. That's why it's a supervised method. You can also do univariate feature selection which is very common. Filter methods; well, we are going to start with all of the features and we're going to select the best subset and we're going to give those to our model and that's going to give us our performance for the model with this subset of our features. One of the ways to visualize this is really correlation matrix. We can look for features where we can see that there is a correlation between two or more of our features. It helps show how features are related, both to each other and with the target variable and to emphasize that, when they're unrelated to each other, that's bad. You only want one of those and of course, you do want it correlated with a target. That's going to fall in a range of correlation between negative 1 and one and one is highly positive correlation and negative 1 is highly negative correlation. Comparing the different tests, you have Pearson's correlation for linear relationships. That's by far I think the most common. Kendall Tau is a rank correlation coefficient, looks at monotonic relationships, and it's usually used with a fairly small sample size for efficiency. Spearman's is another one. It's also rank correlation and it also is looking at monotonic relationship. Other methods; well, there's mutual information. Mutual information has some nice characteristics to it. F-test, that's another fairly common one and chi-squared. Chi-square is also a fairly common one. Looking at how we would do this in code, let's just take a look using Pearson's correlation. This is going to use pandas. We have a data frame and we're using the Pearsoncorrelation.cor method. That gives us our correlation and we can then draw our heat map with seaborn and that's what that's going to look like. That gives us our correlation matrix. Looking at it again, we've got the absolute value of that and we can select our target. That gives us our result with a subset of our features that we've selected. In this case, we've eliminated seven features because they were correlated. Now, we have instead of 30, we have 21 features in our feature vector. The accuracy is actually better, the AUC is actually better, precision is actually better and recall is exactly the same. At least have five decimal places. The F1 score is a little better. Removing features, mostly almost all our metrics improved. That also goes along with the improvements that we made in the compute resources that are required to process 21 features instead of 30. That's our best result so far. Let's look at univariate feature selection. Univariate feature selection and we're going to do that using Sci-kit learn. There's SelectKBest, there's SelectPercentile, GenericUnivariateSelect, which is fairly generic I assume. Some statistical tests we can use with that, there's mutual information and F-tests for regression problems. For classification, we have chi-squared and there's a version of the F-test for classification and similarly for mutual information, there's a version of that for classification. Let's look at how that gets implemented in code. We've got a function that we're going to define for univariate selection. It's going to take our test set, actually our training set has been split for both training and test and then that's going to be scaled with a standard scalar and we're going to use the MinMaxScalar as well to give us a scaled x and then our selector we're going to use is the SelectKBest and we're going to use that with the chi-squared test and that's going to look for the 20 best features here. We're going to fit that transform, we're going to get the features that it selects and we're going to drop the other features in our original data set. That gives us the feature names of the features that we've selected. How does that perform? Well, a univariate test using chi-squared, we asked for 20 features, so it gave us 20 features. The accuracy dropped a little bit, so did the AUC, so did the precision, but recall interestingly did not, its still exactly the same and the F1 score is unfortunately a little bit lower. The correlation is still our best result. It's one more feature, but it's still our best result.

# Wrapper metods
Let's look at some wrapper methods. Wrapper methods work all differently. It stores supervised method, but we're going to use this with a model and there's different ways to do it. But basically you're iterating through, it's a search method against the features that you have using a model as the measure of their effectiveness. We can do it through forward elimination and we'll talk about this in a second. Forward elimination, backward elimination or recurrent feature elimination. Let's take a look at these. We start with all of our feature, regenerate a subset of those features, and we'll talk about how that gets generated, that gets given to our model. The results that is generated from that model is then used to generate the next subset. That becomes this feedback loop to select the best subset of our features using our model as a measure. That gives us the performance of the final best subset that is selected. There's different wrapper methods, forward selection and backwards selection and recursive feature selection or recursive feature elimination rather, these can all be used to select a subset that we just looked at through each iteration of that feedback loop. For forward selection it's an iterative, greedy method. We start with one feature, it's greedy, and then we evaluate the model performance and we add one at a time features. We're adding, we're gradually building up this feature vector, one feature at a time, starting with just one feature. We try to add the next feature that gives the best performance, but we're going to measure that to see what the result is. We keep repeating this until there's no improvement and then we know that we've generated the best subset of our feature. Backward elimination, well, if you think about it, we just looked at forward elimination. Backward elimination starts with all of the features and evaluates the model performance when removing feature. It's exactly what you might think from the name one at a time. We remove the next feature, trying to get to better performance with less features and we keep doing that until there's no improvement. Recursive feature elimination, we select a model to use for evaluating feature importance. We select the desired number of features and we fit them up. We rank the features by importance. We need to have a method of assigning importance to those features and then we discard the least important features. We keep doing that until we get down to the number of features that we're looking to target. An important aspect of this is that we need to have a measurement of feature importance in our model and not all models are able to do that. For recursive feature elimination, this is what the code might look like. Again, we're pulling in our data and splitting it with train and test, and pulling out the labels from our data as well. We have X and Y, why you're going to scale it because it's always a good idea. Then we're going to use a random forest classifier. A random forest classifier is one of the model types where we can get the feature importance. In this case we're using entropy as the metric and we're initializing it with a random state. We're going to apply our random feature elimination trying to get to 20 features with our model that gets fitted. That's going to do that elimination and that results in the list of features that we've selected. Those are the names of the features that we've selected, the 20 features that we were looking for to eliminate all but 20 of our features. Then we're going to use that to do the evaluation to get the final value. How does that look compared to our other examples? Well, so recursive feature elimination also got two of our 20 features. The accuracy was quite a bit better than it was for univariate, in fact, it's as good as correlation. The AUC also as good as correlation, which is great. The precision also as good, I would say as correlation, it doesn't go quite so far out, but that's for precision. For recall, exactly the same, for the F1 score is actually slightly better. I mean, just barely, but slightly better. We've gotten to fewer features, we did it with 20 features instead of 21 for correlation. That recursive feature elimination is now our best result.

# Embedded metods
Now let's look at some Embedded Methods.
Play video starting at ::3 and follow transcript0:03
So, what are embedded methods? Well, it's a again, a supervised method of Features Selection. And we've looked at Filter Methods. We've looked at Wrapper Methods. Let's look at Embedded Methods. So L1 or L2 regularization is essentially an embedded method for doing feature selection. Feature importance is another method. Both of these are highly connected to the model that you're using. So these both L1 regularization and feature importance really sort of an intrinsic characteristic of the model that you're working with. So it assigns scores. And for regularization really, we're talking about waiting for each feature in the data. And it discards features often by by setting those weights to zero or near zero. And that's going to eliminate the features that we're talking about based on the feature essentially the feature important.
Play video starting at :1: and follow transcript1:00
So look at that in SKLearn, if we look at Feature Importance class, that's built into the Tree Based Model. So, again, we're still using the RandomForestClassifier that we've been using all along. That's one of the model types that does include feature importance. It's available as a property of that model, the feature importances. And we can use SelectFromModel to select the features from the train model based on the assigned feature importances. So again, working with the model, it's really a characteristic of the model. How does that look in code? Well, we're going to define a function. Feature importance is from Tree Based Model and here's our data, it's been split into training and test and we've separated our labels. We're going to use our RandomForestClassifier as our model and we're going to fit it. And then we're going to pull out the feature importances here. That's going to give us a series that we're going to use a Panda series. That Panda series, we're going to select the 10 best. So the 10 highest feature importances and show that. So if we do that, this is the visualization that we get and we can see the 10 best features that we have based on the feature importance that was calculated in the model. So using that to select those features, we're going to go back and select from our model. That's going to give us our model and we're going to get support to get the indexes for those features. And then we're going to drop the other features from our feature vector. And that's going to give us the names of the features that we're keeping. So tying those together, we've got feature important from tree based model that that's going to give us the feature importances and plot them. Then we're going to select the features that we're going to keep and that's going to give us our performance. So in this case we've selected 14 features and looking at the metrics for them, they're actually not quite as good but they're pretty good. So accuracy. It's now down a little bit. The ROC is up a little bit. It's kind of back to what it was with all features are almost there. Precision is down a little bit but still I mean it's down a little bit. Recall is not bad really because pretty much the same. It is the same and the F1 Score is back to what it was with all of the features. So Recursive Feature Elimination is really still our best result. Although with Feature Importance we were able to get down to 14 features. So this is a question where we need to consider the importance of our metrics versus the importance of reducing our compute resources. If it's really important, that compute resources, we may want to do that with the 14 features that were selected with Feature Importance. But if we're really interested in maintaining those metrics were improving them, then Recursive Feature Elimination is still the best result.
Play video starting at :4:18 and follow transcript4:18
Such review. We've covered a lot this week, we started out with an Introduction to Preprocessing. We talked at some length about Feature Engineering and why it's so important to doing machine learning, especially in production settings. We looked at Preprocessing Data at Scale, which is going to be important whenever you're working with large amounts of data or large models and specifically at TensorFlow Transform. We also looked at Feature Spaces and understanding how Feature Spaces work and and why we need to consider that as we do Feature Selection. And we looked at different kinds of Feature Selection. So Filter Methods and Wrapper Methods and Embedded Methods. We've covered a lot about working with our data and doing feature engineering. This has been quite a week so far.