# Data journey
Hello, and welcome to our continuing saga of data. This week, we'll be talking about several data related topics, including metadata, which is important for understanding our data over time and are training runs. Will also talk about schemas which helped describe our Data. We'll look at some places where we might want to store our data to make it easier to work with. It's a lot to cover. Let's get started. Welcome to Data journey and Data Storage. Let's begin with the Data journey and all it takes to understand this important process. Understanding data provenance requires understanding the Data journey throughout the life cycle of the production pipeline. More specifically, accounting for the evolution of data and models throughout the process. ML metadata is a versatile library to address these challenges, which helps with debugging and reproducibility. Concretely, it allows us to revisit and track data and model changes as they occur during training. Trying to understand data provenance begins with the Data journey. The journey starts with raw features and labels from whatever sources that we have. The Data describes a function that maps the input in the training set to the labels that we're trying to predict. During training, the model learns the functional mapping from the input to the labels so as to be as accurate as possible. The Data transforms and changes as part of this training process. As the Data flows through the process it transforms. Examples of this are, are changing data formats, applying feature engineering and training the model to make predictions. There's a dual connection between understanding these data transformations and interpreting the model's results. Therefore, it's important to track and document these changes closely. Data artifacts are created as pipeline components execute. What exactly is an artifact? Each time a component produces a result, it generates an artifact. This includes basically everything that is produced by the pipeline, including the data in different stages of transformation, often as a result of feature engineering and the model itself and things like the schema, and metrics and so forth. Basically, everything that's produced, every result that is produced as an artifact. The terms data provenance and data lineage are basically synonyms and they're used interchangeably. Data provenance or lineage is a sequence of the artifacts that are created as we move through the pipeline. Those artifacts are associated with a code and components that we create. Tracking those sequences is really key for debugging and understanding the training process and comparing different training runs that may happen months apart. Data provenance matters a great deal and it helps us to understand the pipeline and to perform debugging. Debugging and understanding requires inspecting those artifacts at each point in the training process, which can help us understand how those artifacts were created and what the results actually mean. Provenance will also allow you to track back through a training run from any point in the process. Also, provenance makes it possible to compare training runs, and understand why they produce different results. Under GDPR or the general data protection regulation, organizations are accountable for the origin, changes and location of personal data. Personal data is highly sensitive, so tracking the origins and changes along the pipeline are key for compliance. Data lineage is a great way for businesses and organizations to quickly determined how the Data has been used and which Transformations were performed as the Data moved through the pipeline. Data provenance is key to interpreting model results. Model understanding is related to this, but it's only part of the picture. The model itself is an expression of the data in the training set. In some sense, we can look at the model as a transformation of the Data. Provenance also helps us understand how the model evolves as it is trained and perhaps optimized. Let's add an important ingredient here, tracking different Data versions. Managing a data pipelines is a big challenge as data evolves through the natural life cycle of a project, over many different training runs. A Machine learning when it's done properly, should produce results that can be reproduced fairly consistently. There will naturally be some variance, but the results should be closed. Code version control is probably something you're familiar with. GitHub is one of the most popular cloud-based code repositories, and there are others as well. Environment versioning is also important. Tools like Docker and terraform help us create repeatable environments and configuration. However, data versioning is also important, and plays a significant role for tracking provenance and lineage data versioning. It's essentially version control for data files so that you can trace changes over time and restore previous versions early. But the tools are somewhat different. For one thing, because of the size of files that we deal with, which are typically or can be any way, much larger than a code file would ever be. Tools for data versioning are just starting to become available. These include DVC and Git LFS. LFS for large files Storage.

# ML metadata
Now we're going to discuss metadata. Something to think about as we discuss this topic are the legal and regulatory considerations. As machine learning becomes increasingly used to make important business decisions, healthcare decisions and financial decisions, legal liability becomes a factor. Being able to interpret a model and being able to trace back the lineage or provenance of the data that was used to train the model become increasingly important to limiting exposure. Okay, let's go ahead and get started.
Play video starting at ::40 and follow transcript0:40
Now let's start exploring how ML metadata or MLMD can help you with tracking artifacts and pipeline changes during a production life cycle. Every run of a production ML pipeline generates metadata containing information about the various pipeline components and their executions or training runs and the resulting artifacts. For example, trained models. In the event of unexpected pipeline behavior or errors, this metadata can be leveraged to analyze the lineage of pipeline components and to help you debug issues. Think of this metadata as the equivalent of logging in software development. MLMD helps you understand and analyze all the interconnected parts of your ML pipeline, instead of analyzing them in isolation. Now consider the two stages in ML engineering that you've seen so far. First you've done data validation, then you've passed the results onto data transformation or feature engineering. This is the first part of any model training process.
Play video starting at :1:57 and follow transcript1:57
But what if you had a centralized repository where every time you run a component, you store the result or the update or any other output of that stage into a repository. So whenever any changes made, which causes a different result, you don't need to worry about the progress you've made so far getting lost. You can examine your previous results to try to understand what happened and make corrections or take advantage of improvements. Let's take a closer look. In addition to the executor where your code runs, each component also includes two additional parts, the driver and publisher. The executor is where the work of the component is done and that's what makes different components different. Whatever input is needed for the executor, is provided by the driver, which gets it from the metadata store. Finally, the publisher will push the results of running the executor back into the metadata store. Most of the time, you won't need to customize the driver or publisher. Creating custom components is almost always done by creating a custom executor. Now, let's look specifically at ML Metadata or MLMD. MLMD is a library for tracking and retrieving metadata associated with ML developer and data scientist workflows. MLMD can be used as an integral part of an ML pipeline or it can be used independently. However, when integrated with an ML pipeline, you may not even explicitly interact with MLMD. Objects which are stored in MLMD are referred to as artifacts. MLMD stores the properties of each artifact in a relational database and stores large objects like data sets on disc or in a file system or block store. When you're working with ML metadata, you need to know how data flows between different successive components. Each step in this data flow is described through an entity that you need to be familiar with. At the highest level of MLMD, there are some data entities that can be considered as units.
Play video starting at :4:22 and follow transcript4:22
First, there are artifacts. An artifact is an elementary unit of data that gets fed into the ML metadata store and as the data is consumed as input or generated as output of each component. Next there are executions. Each execution is a record of any component run during the ML pipeline workflow, along with its associated runtime parameters. Any artifact or execution will be associated with only one type of component. Artifacts and executions can be clustered together for each type of component separately. This grouping is referred to as the context. A context may hold the metadata of the projects being run, experiments being conducted, details about pipelines, etc. Each of these units can hold additional data describing it in more detail using properties. Next there are types, previously, you've seen several types of units that get stored inside the ML metadata. Each type includes the properties of that type. Lastly, we have relationships. Relationships store the various units getting generated or consumed when interacting with other units. For example, an event is the record of a relationship between an artifact and an execution. So, ML metadata stores a wide range of information about the results of the components and execution runs of a pipeline. It stores artifacts and it stores the executions of each component in the pipeline. It also stores the lineage information for each artifact that is generated. All of this information is represented in metadata objects and this metadata is stored in a back end storage solution. Let's take a look at the architecture of ML metadata or MLMD. On the top are the various components present in any ML pipeline. All of these components are individually connected to a centralized metadata store of ML metadata, so that each component can independently access the metadata at any stage of the pipeline.
Play video starting at :6:49 and follow transcript6:49
An ML pipeline may optionally have a GUI console that can access the data from the metadata store directly to track the progress of each component. At the heart of the metadata store is the artifact which is described by its corresponding artifact type. Artifacts become the inputs to any pipeline components which depend on them. And the corresponding use of artifacts by components is recorded in executions. The input of an artifact into a component is described by an input event and the corresponding output of a new artifact from the component is described by an output event. This interaction between artifacts and executions is represented by context through the relationships of attribution and association. Lastly, all of the data generated by the metadata store is stored in various types of back end storage like SQLite and MySQL and large objects are stored in a file system or block store.
Play video starting at :8:2 and follow transcript8:02
Let's review. You learned a lot about the architecture and nomenclature of ML metadata or MLMD and the artifacts and entities which it contains. This should give you some idea of how you can leverage MLMD to track metadata and the results flowing through your pipeline to better understand your training process, both now and in previous training runs of your pipeline.

# ML metadata in action
Now, let's take a look at using ML Metadata with a coding example. Besides data lineage and provenance tracking, you get several other benefits through ML Metadata. This includes the ability to construct a directed acyclic graph or DAG, of the component executions occurring in a pipeline, which can be useful for debugging purposes. Through this, you can verify which inputs have been used in an execution. You can also summarize all the artifacts belonging to a specific type generated after a series of experiments. For example, you can list all of the models that have been trained. You can then compare them to evaluate your various training runs. Now, let's get started with writing code for ML Metadata. You may have to install ML Metadata, which you can do using PIP as shown here. Then there are two imports from ML Metadata which are used frequently, the ML Metadata store itself and the ML Metadata store PB2, which is a protocol buffer or protobuf. Start by setting up the storage backend. ML Metadata store is the database where ML Metadata registers all of the metadata associated with your project. ML Metadata provides APIs to connect with a fake database for quick prototyping, SQLite and MySQL. We also need a block store or file system where ML Metadata stores large objects like datasets. Let's quickly explore the first three options. For any storage backend, you'll need to create a connection config object using the metadata store PB2 protobuf. Then, based on your choice of storage backend, you need to configure this connection object. Here you're signaling that your fake database is in parent, which is the primary memory of the system on which it's running. Finally, you create the store object passing in the connection config. Regardless of what storage or database you use, the store object is a key part of how you interact with ML Metadata. To use SQLite, you start by creating a connection config again then you configure the connection config object with the location of your SQLite file. Make sure to provide that with the relevant read and write permissions based on your application. Finally, you create the store object passing in the connection config. As you might imagine, using MySQL is very similar, you start by creating a connection config. Your connection config object should be configured with the relevant host name, the port number, the database location, username, and password of your MySQL database user and that's shown here. Finally, you create the store object passing in the connection config. Now, that the storage backend is configured, it's time to use ML Metadata to solve a problem. Let's go over a workflow using TFTV. We're going to use that in conjunction with ML Metadata. The choice here is a tabular dataset containing many features. In the lab, ML Metadata is explicitly programmed because in a full-fledged ML Pipeline, ML Metadata is intrinsically capable enough to understand the flow of data between various components and perform its necessary duties. To help you better understand ML Metadata, you'll be using it outside of a pipeline. The lab also shows ML Metadata integration with TensorFlow Data Validation or TFTP. By the end of the lab, you should have some intuition about how ML Metadata can keep track of progress and how you can use ML Metadata to track your training process and pipeline. Let's review the key points which we covered in this lesson. First, you learned about data lineage and provenance to address data evolution over the ML Pipeline lifecycle. Then you went over metadata for tracking those data changes. Then you inspected the architecture of the ML Metadata library. Finally, in the ungraded lab, you read an example to register artifacts, executions, and contexts.

# Schema development
Now let's discuss schema development. In this lesson on evolving data, you'll learn about developing enterprise schema environments and how to iteratively create and maintain enterprise data schemas. You'll be using schemas quite extensively. Let's quickly review what a schema is and how helpful they are in the context of production ML. Schemas are relational objects summarizing the features in a given dataset or project. They include the feature name, the feature of variable type. For example, an integer, float, string or categorical variable. Whether or not the feature is required. The valency of the feature, which applies to features with multiple values like lists or array features, and expresses the minimum and maximum number of values. Information about the range and categories and feature default values. Schemas are important, as your data and feature set evolves over time. From your experience, you know that data keeps changing and this change often results in change distributions. Let's focus on how to observe data that has changed as it keeps evolving. Changing data often results in a new schema being generated. However, there are some special use cases. Imagine that even before you assess the dataset, you have an idea or information about the expected range of values for your features. The initial dataset that you've received is covering only a small range of those values. In that case, it makes sense to adjust or create your schema to reflect the expected range of values for your features. A similar situation may exist for the expected values of categorical features. Besides that, your schema can help you find problems or anomalies in your dataset, such as missing required values or values of the wrong type. All these factors have to be considered when you're designing the schema of your ML pipeline. As data keeps evolving, there are some requirements which must be met in production deployments. Let's consider some important ones. The first factor is reliability. The ML platform of your choice should be resilient to disruptions from inconsistent data. There's no guarantee that you'll receive clean and consistent data every time. In fact, I can almost guarantee you that you won't. Your system needs to be designed to handle that efficiently. Also, your software might generate unexpected runtime errors and your pipeline needs to gracefully handle that also. Problems with misconfiguration should be detected and handled gracefully. Above all, the orchestration of execution among different components in the pipeline should happen smoothly and efficiently. Another factor to consider in your ML pipeline platform is scalability during data evolution. During training, your pipeline may need to handle a large amount of training data well, including keeping expensive accelerators like GPUs and TPUs busy. When you serve your model, especially in online scenarios such as running on a server, you'll almost always have varying levels of request traffic. Your infrastructure needs to scale up and down to meet those latency requirements while minimizing the cost. If your system isn't designed to handle data evolution, it will quickly run into problems. These include the introduction of anomalies in your dataset. Will your system detect those anomalies? Your system and your development process should be designed to treat data errors as first-class citizens in the same way that bugs in your code are treated. In some cases, those anomalies are alerting you that you need to update the schema and accommodate valid changes in your data. The evolution of your schemas can be a useful tool to understand and track the evolution of your data. Also, schemas can be used as key inputs into automated processes that work with your data like automatic feature engineering.

# Schema environments
Let's discuss schema environments. Your business and data will evolve throughout the lifetime of your production pipeline. It's often the case that as your data evolves, your schema evolves also. As you're developing your code to handle changes in your schema, you may have multiple versions of your schema all active at the same time. You may have one schema being used for development, one currently in test, and another currently in production. Having version control for your schemas, just like you do for your code, helps make this manageable. In some cases, you may need to have different schemas to support multiple training and deployment scenarios for different data environments. For example, you may want to use the same model on a server and in a mobile application but imagine that a particular feature is different in those two environments. Maybe in one case it's an integer and the other case it's a float. You need to have a different schema for each to reflect the difference in the data. Along with that, your data's evolving. Potentially, in all your different data environments at once. But at the same time you also needed to check your data for problems or anomalies, and schemas are a key part of checking for anomalies. Let's look at an example of how a schema can help you detect errors in your serving request data and why multiple versions of your schema are important. We'll start by inferring the serving schema and we'll use TensorFlow Data Validation or TFTV to do that. Then we're going to generate statistics for the serving dataset. Then we'll use TFTV to find if there are any problems with this data and visualize the result in a notebook. Look at that. TFTV reports back that there are anomalies in the serving data. Since this is a dataset that contains prediction requests, that's actually not surprising. The label which is cover type is missing, but the schema is telling TFTV that the cover type feature is required. So it's flagging this as an anomaly. How do we fix this problem? In scenarios where you need to maintain multiple types of the same schemas, you often need to keep the schema environment. This is most commonly true of the difference between training and serving data. You can choose to customize your schema based on the situation you're going to handle. For example, in this case, the setup is to maintain two schemas. One for training data, where the label is required and the other for serving where we know we won't have the label. The code for multiple schema environments is fairly straightforward. In our existing environment, we already have a schema for training. We then create two named environments called training and serving. We modify our serving environment by removing the cover type feature. Since we know that in serving, we won't have that in our feature set. Lastly, the code sets up the serving environment and uses it to validate the serving data. Now, there are no anomalies found since we're using the correct schema for our data. Let's review. First we discussed how to iteratively update and fine tune your schema to adapt to evolving data. Then we focused on reliability and scalability during the evolution cycle of data. Then you implemented schema environments to deal with anomalies in your serving data.

# Feature stores
In these next three topics will be talking about data storage. Now there are no assignments for you to turn in for these three segments, but I highly encourage you to take a look. There's some great information here and it's becoming increasingly important, especially the part about feature stores. But depending upon where you are and what you already have, you may have a data warehouse or data lake that you want to leverage. So understanding those becomes important. Okay, let's get started. Now let's turn to the question of where we should store our data. We'll start with a discussion of feature stores. A feature store is a central repository for storing documented, curated and access controlled features. Using a feature store enables teams to share, discover and use highly curated features. A feature store makes it easy to discover and consume that feature and that can be both online or offline for both serving and training. But people often discover is that many modeling problems use identical or similar features. So often the same data is used in multiple modeling scenarios. In many cases, a feature store can be seen as the interface between feature engineering and model development. Feature stores are valuable centralized feature repositories that reduce redundant work. They are also valuable because they enable teams to share data and discover data that is already available. You may have different teams in an organization with different business problems that they're trying to solve. But they're using identical data or data that's very similar. For these reasons, feature stores are becoming the predominant choice for enterprise data storage. For machine learning, for large projects and organizations. Feature stores often allow transformations of data so that you can avoid duplicating that processing in different individual pipelines. The access to the data in feature stores can be controlled based on role based permissions. The data in the feature stores can be aggregated to form new features. It can also be anonymous sized and even purged for things like wipeouts For GDP our compliance, for example. Feature stores typically allow for feature processing offline that can be on a regular basis, maybe on a cron job, for example. Imagine that you're going to run a job to ingest data. And then maybe do some feature engineering on it and produce additional features from it. Maybe for feature crosses, for example, these new features will also be published to the Feature store. You can also integrate that with monitoring tools as your processing and adjusting your data. You could be running monitoring again offline. Those processed features are stored for offline use. They can also be part of a prediction request. But doing a join with the data provided in the prediction request to pull in additional information. Feature metadata allows you to discover the features that you need. The metadata that describes the data that you are keeping is a tool. And often the main tool for trying to discover the data that you're looking for. For online feature usage where predictions must be returned in real time. The latency requirements are typically fairly strict. You're going to need to make sure that you have fast access to that data. If you're going to do a join, for example, maybe with user account information along with individual requests. That join has to happen quickly. That's good, but it's often difficult to compute some of those features in a performant manner online. So having pre computed features is often a good idea. If you pre compute and store those features then you can use them later. And typically that's at fairly low latency. You can also do this in a batch environment. Again, you don't want the latency to be too long, but it probably isn't as strict as an online request. For pre computing and loading, especially things like historical features, it tends to be fairly simple. For historical features in a badger environment, it's also usually straightforward. However, when you're training your model, you need to make sure that you only include data that will be available at the time that a serving request is made. Including data that is only available at some time after a serving request is referred to as time travel. And many feature stores include safeguards to avoid that. Besides it violates the laws of physics and we don't want to do that. You might do pre computing on a clock every few hours or once a day. You're going to use of course, that same data for both training and serving in order to avoid training, serving skew. The goals of most feature stores are providing a unified means of managing featured data. They can scale from a single person up to large enterprises. It needs to be performant and you want to try to use that same data, both when you're training and serving your models. You want consistency and also point in time correct access to feature data. You want to avoid making a prediction, for example, using data that will only be available in the future when you're serving your model. In other words, if you're trying to predict something that will happen tomorrow. You want to make sure that you are not including data from tomorrow. It should only be data from before tomorrow. Most feature stores provide tools to enable discovery and to allow you to document and provide insights into your features.

# Data warehouse
Now we're going to discuss data warehouses. Fun fact. I once worked in a warehouse, but it wasn't a data warehouse. It was an industrial tool warehouse. Anyway, let's get started. Data warehouses. Now let's discuss another popular enterprise data storage solution. Data warehouses. Data warehouses were originally developed for big data and business intelligence applications, but they're also valuable tools for production ML. A data warehouse is a technology that aggregates data from one or more sources so that it can be processed and analyzed. A data warehouse is usually meant for long running batch jobs and their storage is optimized for read operations. Data entering into the warehouse may not be in real time. When you're storing data in a data warehouse, your data needs to follow a consistent schema. Let's look at some key features of data warehouses. A data warehouse is subject oriented and the information that's stored in it revolves around a topic. For example, data stored in a data warehouse may be focused on the organization's customers or vendors, or etc. The data in data warehouse may be collected from multiple types of sources, such as relational databases or files and so forth. The data collected in a data warehouse is usually timestamped to maintain the context of when it was generated. Data warehouses are nonvolatile, which means the previous versions of data are not erased when new data is added. That means that you can access the data stored in a data warehouse as a function of time and understand how that data has evolved. Let's look at some advantages of data warehouses. First, data warehouses offer enhanced ability to analyze your data by time stamping your data. A data warehouse can help maintain contexts. When you store your data in a data warehouse, it follows a consistent schema and that helps improve the data quality and consistency. Studies have shown that the return on investment for data warehouses tend to be fairly high for many use cases. Lastly, the read and query efficiency from data warehouses is typically high, giving you fast access to your data. You're probably familiar with databases. A natural question is, what's the difference between a data warehouse and a database? Here are some comparisons. Data warehouses are meant for analyzing data, whereas databases are often used for transaction purposes. Inside a data warehouse, there may be a delay between storing the data and the data getting reflected in the system. But in a database, data is usually available immediately after it's stored. Data warehouses store data as a function of time, and therefore, historical data is also available. Data warehouses are typically capable of storing a larger amount of data compared to databases. Queries in data warehouses are complex in nature and tend to run for a long time. Whereas queries in database are simple and tend to run in real time. Normalization is not necessary for data warehouses, but it should be used with databases.

# Data lakes
Now, let's discuss another popular enterprise storage solution, data lakes. A data lake is a system or repository of data stored in its natural and raw format, which is usually in the form of blobs or files. A data lake, like a data warehouse, aggregates data from various sources of enterprise data. A data lake can include structured data like racial databases or semi-structured data like CSV files, or unstructured data like a collection of images or documents, and so forth. Since data lake store data in its raw format, they don't do any processing, and they usually don't follow a schema. How does a data lake differ from a data warehouse? Let's compare the two. The primary difference between them is that in a data warehouse, data is stored in a consistent format which follows a schema, whereas in data lakes, the data is usually in its raw format. In data lakes, the reason for storing the data is often not determined ahead of time. This is usually not the case for a data warehouse, where it's usually stored for a particular purpose. Data warehouses are often used by business professionals as well, whereas data lakes are typically used only by data professionals such as data scientists. Since the data in data warehouses is stored in a consistent format, changes to the data can be complex and costly. Data lakes however are more flexible, and make it easier to make changes to the data. Let's review our discussion of enterprise data storage. First, you learned about feature stores, which are repositories for highly curated feature data, specifically for use in machine learning. You learned about subject-oriented read-optimized data repositories, known as data warehouses. Finally, you explored data lakes, which are repositories for raw and unprocessed data. We've come to the end of another week and we talked about a lot of great stuff this week. ML metadata, working with storage for features stores, schemas, data lakes, and data warehouses. I hope you had fun. I sure did. We'll see you next time.
